{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2450f0f8-2303-413e-bed5-b2a8a7e90581",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 1: Creating Delta Table using Three Methods\n",
    "1. Load the given CSV and JSON datasets into Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d534f4c0-f5de-498f-af54-761be17cfdc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/new_employee_data4.csv\", \"dbfs:/FileStore/streaming/input/new_employee_data.csv\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakesAssignment\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "566c8c30-78a4-4d3c-9953-151cc1b2577b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Create a Delta table using the following three methods:\n",
    "Create a Delta table from a DataFrame.\n",
    "Use SQL to create a Delta table.\n",
    "Convert both the CSV and JSON files into Delta format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee53a4e5-76a7-49de-ac6a-76dc0cb63cde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: Create a Delta table from a DataFrame\n",
    "employee_data_df = spark.read.csv(\"dbfs:/FileStore/streaming/input/employee_data.csv\", header=True, inferSchema=True)\n",
    "employee_data_df.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Shared/employee_data_table\")\n",
    "\n",
    "\n",
    "# Method 2: Use SQL to create a Delta table\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE employees_sql\n",
    "USING DELTA\n",
    "AS SELECT * FROM delta.`/Workspace/Shared/employee_data_table`\n",
    "\"\"\")\n",
    "\n",
    "# Method 3: Convert both the CSV and JSON files into Delta format\n",
    "# For CSV\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee_data4.csv\", \"dbfs:/FileStore/streaming/input/employee_data.csv\")\n",
    "\n",
    "# For JSON\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/products4.json\", \"dbfs:/FileStore/streaming/input/products.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f207608e-5ec1-4504-972e-824043a3fd46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 2: Merge and Upsert (Slowly Changing Dimension - SCD)\n",
    "1. Load the Delta table for employees created in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd396cb7-18d0-4653-8689-576702d5796e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmployeeID</th><th>EmployeeName</th><th>Department</th><th>JoiningDate</th><th>Salary</th></tr></thead><tbody><tr><td>101</td><td>John</td><td>HR</td><td>2023-01-10</td><td>50000</td></tr><tr><td>102</td><td>Alice</td><td>Finance</td><td>2023-02-15</td><td>70000</td></tr><tr><td>103</td><td>Mark</td><td>Engineering</td><td>2023-03-20</td><td>85000</td></tr><tr><td>104</td><td>Emma</td><td>Sales</td><td>2023-04-01</td><td>55000</td></tr><tr><td>105</td><td>Liam</td><td>Marketing</td><td>2023-05-12</td><td>60000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "John",
         "HR",
         "2023-01-10",
         "50000"
        ],
        [
         "102",
         "Alice",
         "Finance",
         "2023-02-15",
         "70000"
        ],
        [
         "103",
         "Mark",
         "Engineering",
         "2023-03-20",
         "85000"
        ],
        [
         "104",
         "Emma",
         "Sales",
         "2023-04-01",
         "55000"
        ],
        [
         "105",
         "Liam",
         "Marketing",
         "2023-05-12",
         "60000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmployeeID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmployeeName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JoiningDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/streaming/input/employee_data.csv\")\n",
    "display(employee_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58040ea5-cfb1-4ead-b608-5b98e1de13b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Merge the new employee data into the employees Delta table.\n",
    "3. If an employee exists, update their salary. If the employee is new, insert\n",
    "their details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28752158-07e2-41f3-b494-59aa7a22ca54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Load the new employee data\n",
    "new_employees_df = spark.read.csv(\"dbfs:/FileStore/streaming/input/new_employee_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Path to the existing employees table\n",
    "employee_data_path = \"dbfs:/FileStore/streaming/input/employee_data\"\n",
    "\n",
    "# convert it to Delta\n",
    "employee_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/streaming/input/employee_data.csv\")\n",
    "employee_data_df.write.format(\"delta\").mode(\"overwrite\").save(employee_data_path)\n",
    "deltaTable = DeltaTable.forPath(spark, employee_data_path)\n",
    "\n",
    "# Merge the new data into the existing Delta table\n",
    "deltaTable.alias(\"old\").merge(\n",
    "    new_employees_df.alias(\"new\"),\n",
    "    \"old.EmployeeID = new.EmployeeID\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"salary\": \"new.salary\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"EmployeeID\": \"new.EmployeeID\",\n",
    "    \"EmployeeName\": \"new.EmployeeName\",\n",
    "    \"Department\": \"new.Department\",\n",
    "    \"JoiningDate\": \"new.JoiningDate\",\n",
    "    \"Salary\": \"new.Salary\"\n",
    "}).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c71b9a2-a66c-42f0-987c-92ca796fe59d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 3: Internals of Delta Table\n",
    "\n",
    "1. Explore the internals of the employees Delta table using Delta Lake features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbd9a52-cef1-42ba-9c12-cfa45deda517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>4</td><td>2024-09-17T05:37:31Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0)</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>3</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 2, numRemovedBytes -> 2790, p25FileSize -> 1505, numDeletionVectorsRemoved -> 1, minFileSize -> 1505, numAddedFiles -> 1, maxFileSize -> 1505, p75FileSize -> 1505, p50FileSize -> 1505, numAddedBytes -> 1505)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr><tr><td>3</td><td>2024-09-17T05:37:28Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>MERGE</td><td>Map(predicate -> [\"(cast(EmployeeID#11432 as int) = EmployeeID#11039)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}])</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1378, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 4042, materializeSourceTimeMs -> 189, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2334, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1470)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr><tr><td>2</td><td>2024-09-17T05:37:23Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1412)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr><tr><td>1</td><td>2024-09-17T05:35:47Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1412)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr><tr><td>0</td><td>2024-09-17T05:34:57Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1412)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4,
         "2024-09-17T05:37:31Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "OPTIMIZE",
         {
          "auto": "true",
          "batchId": "0",
          "clusterBy": "[]",
          "predicate": "[]",
          "zOrderBy": "[]"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         3,
         "SnapshotIsolation",
         false,
         {
          "maxFileSize": "1505",
          "minFileSize": "1505",
          "numAddedBytes": "1505",
          "numAddedFiles": "1",
          "numDeletionVectorsRemoved": "1",
          "numRemovedBytes": "2790",
          "numRemovedFiles": "2",
          "p25FileSize": "1505",
          "p50FileSize": "1505",
          "p75FileSize": "1505"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ],
        [
         3,
         "2024-09-17T05:37:28Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "MERGE",
         {
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedBySourcePredicates": "[]",
          "notMatchedPredicates": "[{\"actionType\":\"insert\"}]",
          "predicate": "[\"(cast(EmployeeID#11432 as int) = EmployeeID#11039)\"]",
          "statsOnLoad": "false"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "4042",
          "materializeSourceTimeMs": "189",
          "numOutputRows": "2",
          "numSourceRows": "2",
          "numTargetBytesAdded": "1378",
          "numTargetBytesRemoved": "0",
          "numTargetChangeFilesAdded": "0",
          "numTargetDeletionVectorsAdded": "1",
          "numTargetDeletionVectorsRemoved": "0",
          "numTargetDeletionVectorsUpdated": "0",
          "numTargetFilesAdded": "1",
          "numTargetFilesRemoved": "0",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "1",
          "numTargetRowsMatchedDeleted": "0",
          "numTargetRowsMatchedUpdated": "1",
          "numTargetRowsNotMatchedBySourceDeleted": "0",
          "numTargetRowsNotMatchedBySourceUpdated": "0",
          "numTargetRowsUpdated": "1",
          "rewriteTimeMs": "1470",
          "scanTimeMs": "2334"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ],
        [
         2,
         "2024-09-17T05:37:23Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         1,
         "WriteSerializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "1412",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ],
        [
         1,
         "2024-09-17T05:35:47Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         0,
         "WriteSerializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "1412",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ],
        [
         0,
         "2024-09-17T05:34:57Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         null,
         "WriteSerializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "1412",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>EmployeeID</th><th>EmployeeName</th><th>Department</th><th>JoiningDate</th><th>Salary</th></tr></thead><tbody><tr><td>count</td><td>6</td><td>6</td><td>6</td><td>6</td><td>6</td></tr><tr><td>mean</td><td>103.5</td><td>null</td><td>null</td><td>null</td><td>62500.0</td></tr><tr><td>stddev</td><td>1.8708286933869707</td><td>null</td><td>null</td><td>null</td><td>15545.631755148024</td></tr><tr><td>min</td><td>101</td><td>Alice</td><td>Engineering</td><td>2023-01-10</td><td>50000</td></tr><tr><td>max</td><td>106</td><td>Olivia</td><td>Sales</td><td>2023-06-10</td><td>85000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         "6",
         "6",
         "6",
         "6",
         "6"
        ],
        [
         "mean",
         "103.5",
         null,
         null,
         null,
         "62500.0"
        ],
        [
         "stddev",
         "1.8708286933869707",
         null,
         null,
         null,
         "15545.631755148024"
        ],
        [
         "min",
         "101",
         "Alice",
         "Engineering",
         "2023-01-10",
         "50000"
        ],
        [
         "max",
         "106",
         "Olivia",
         "Sales",
         "2023-06-10",
         "85000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmployeeID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmployeeName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JoiningDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmployeeID</th><th>EmployeeName</th><th>Department</th><th>JoiningDate</th><th>Salary</th></tr></thead><tbody><tr><td>101</td><td>John</td><td>HR</td><td>2023-01-10</td><td>50000</td></tr><tr><td>103</td><td>Mark</td><td>Engineering</td><td>2023-03-20</td><td>85000</td></tr><tr><td>104</td><td>Emma</td><td>Sales</td><td>2023-04-01</td><td>55000</td></tr><tr><td>105</td><td>Liam</td><td>Marketing</td><td>2023-05-12</td><td>60000</td></tr><tr><td>102</td><td>Alice</td><td>Finance</td><td>2023-02-15</td><td>75000 # Updated Salary</td></tr><tr><td>106</td><td>Olivia</td><td>HR</td><td>2023-06-10</td><td>65000 # New Employee</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "John",
         "HR",
         "2023-01-10",
         "50000"
        ],
        [
         "103",
         "Mark",
         "Engineering",
         "2023-03-20",
         "85000"
        ],
        [
         "104",
         "Emma",
         "Sales",
         "2023-04-01",
         "55000"
        ],
        [
         "105",
         "Liam",
         "Marketing",
         "2023-05-12",
         "60000"
        ],
        [
         "102",
         "Alice",
         "Finance",
         "2023-02-15",
         "75000 # Updated Salary"
        ],
        [
         "106",
         "Olivia",
         "HR",
         "2023-06-10",
         "65000 # New Employee"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmployeeID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmployeeName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JoiningDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Describe the history of the employees Delta table to understand its evolution\n",
    "history_df = deltaTable.history()\n",
    "display(history_df)\n",
    "\n",
    "# Describe the details of the employees Delta table to understand its schema and properties\n",
    "details_df = deltaTable.toDF()\n",
    "display(details_df.describe())\n",
    "\n",
    "\n",
    "# List all the files associated with the employees Delta table\n",
    "files_df = deltaTable.toDF()\n",
    "display(files_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd2c73e1-c272-4aae-b665-5e0dda2c6c3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2.Check the transaction history of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb03ab6-62e4-42ee-8da7-4f759daeddf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>4</td><td>2024-09-17T05:37:31Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0)</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>3</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 2, numRemovedBytes -> 2790, p25FileSize -> 1505, numDeletionVectorsRemoved -> 1, minFileSize -> 1505, numAddedFiles -> 1, maxFileSize -> 1505, p75FileSize -> 1505, p50FileSize -> 1505, numAddedBytes -> 1505)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr><tr><td>3</td><td>2024-09-17T05:37:28Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>MERGE</td><td>Map(predicate -> [\"(cast(EmployeeID#11432 as int) = EmployeeID#11039)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}])</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1378, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 4042, materializeSourceTimeMs -> 189, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2334, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1470)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr><tr><td>2</td><td>2024-09-17T05:37:23Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1412)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr><tr><td>1</td><td>2024-09-17T05:35:47Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1412)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr><tr><td>0</td><td>2024-09-17T05:34:57Z</td><td>8532116595080723</td><td>azuser2110_mml.local@techademy.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(1284719369368105)</td><td>0911-102441-r0fo913u</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1412)</td><td>null</td><td>Databricks-Runtime/15.4.x-photon-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4,
         "2024-09-17T05:37:31Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "OPTIMIZE",
         {
          "auto": "true",
          "batchId": "0",
          "clusterBy": "[]",
          "predicate": "[]",
          "zOrderBy": "[]"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         3,
         "SnapshotIsolation",
         false,
         {
          "maxFileSize": "1505",
          "minFileSize": "1505",
          "numAddedBytes": "1505",
          "numAddedFiles": "1",
          "numDeletionVectorsRemoved": "1",
          "numRemovedBytes": "2790",
          "numRemovedFiles": "2",
          "p25FileSize": "1505",
          "p50FileSize": "1505",
          "p75FileSize": "1505"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ],
        [
         3,
         "2024-09-17T05:37:28Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "MERGE",
         {
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedBySourcePredicates": "[]",
          "notMatchedPredicates": "[{\"actionType\":\"insert\"}]",
          "predicate": "[\"(cast(EmployeeID#11432 as int) = EmployeeID#11039)\"]",
          "statsOnLoad": "false"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "4042",
          "materializeSourceTimeMs": "189",
          "numOutputRows": "2",
          "numSourceRows": "2",
          "numTargetBytesAdded": "1378",
          "numTargetBytesRemoved": "0",
          "numTargetChangeFilesAdded": "0",
          "numTargetDeletionVectorsAdded": "1",
          "numTargetDeletionVectorsRemoved": "0",
          "numTargetDeletionVectorsUpdated": "0",
          "numTargetFilesAdded": "1",
          "numTargetFilesRemoved": "0",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "1",
          "numTargetRowsMatchedDeleted": "0",
          "numTargetRowsMatchedUpdated": "1",
          "numTargetRowsNotMatchedBySourceDeleted": "0",
          "numTargetRowsNotMatchedBySourceUpdated": "0",
          "numTargetRowsUpdated": "1",
          "rewriteTimeMs": "1470",
          "scanTimeMs": "2334"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ],
        [
         2,
         "2024-09-17T05:37:23Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         1,
         "WriteSerializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "1412",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ],
        [
         1,
         "2024-09-17T05:35:47Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         0,
         "WriteSerializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "1412",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ],
        [
         0,
         "2024-09-17T05:34:57Z",
         "8532116595080723",
         "azuser2110_mml.local@techademy.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "1284719369368105"
         ],
         "0911-102441-r0fo913u",
         null,
         "WriteSerializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "1412",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-photon-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the transaction history of the employees Delta table\n",
    "transaction_history_df = deltaTable.history()\n",
    "display(transaction_history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a3ad7fc-1c8a-4045-b2b2-0d58c6d85ea9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Perform Time Travel and retrieve the table before the previous merge operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787a2187-9906-4761-b9d9-7c4554b152b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmployeeID</th><th>EmployeeName</th><th>Department</th><th>JoiningDate</th><th>Salary</th></tr></thead><tbody><tr><td>101</td><td>John</td><td>HR</td><td>2023-01-10</td><td>50000</td></tr><tr><td>103</td><td>Mark</td><td>Engineering</td><td>2023-03-20</td><td>85000</td></tr><tr><td>104</td><td>Emma</td><td>Sales</td><td>2023-04-01</td><td>55000</td></tr><tr><td>105</td><td>Liam</td><td>Marketing</td><td>2023-05-12</td><td>60000</td></tr><tr><td>102</td><td>Alice</td><td>Finance</td><td>2023-02-15</td><td>75000 # Updated Salary</td></tr><tr><td>106</td><td>Olivia</td><td>HR</td><td>2023-06-10</td><td>65000 # New Employee</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "John",
         "HR",
         "2023-01-10",
         "50000"
        ],
        [
         "103",
         "Mark",
         "Engineering",
         "2023-03-20",
         "85000"
        ],
        [
         "104",
         "Emma",
         "Sales",
         "2023-04-01",
         "55000"
        ],
        [
         "105",
         "Liam",
         "Marketing",
         "2023-05-12",
         "60000"
        ],
        [
         "102",
         "Alice",
         "Finance",
         "2023-02-15",
         "75000 # Updated Salary"
        ],
        [
         "106",
         "Olivia",
         "HR",
         "2023-06-10",
         "65000 # New Employee"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmployeeID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmployeeName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JoiningDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform Time Travel to retrieve the table before the previous merge operation\n",
    "version_before_merge = deltaTable.history().filter(\"operation = 'MERGE'\").select(\"version\").collect()[0][0]\n",
    "table_path = \"dbfs:/FileStore/streaming/input/employee_data\" \n",
    "df_before_merge = spark.read.format(\"delta\").option(\"versionAsOf\", version_before_merge).load(table_path)\n",
    "display(df_before_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "319e6758-893f-4dc6-97c0-1dacd83bc202",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 4: Optimize Delta Table\n",
    "1. Optimize the employees Delta table for better performance.\n",
    "2. Use Z-ordering on the Department column for improved query performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d0e8c4-019a-442c-ae2f-5add042978e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Access the Delta table\n",
    "deltaTable = DeltaTable.forName(spark, \"employee_data\")\n",
    "\n",
    "# Optimize and execute Z-order by Department\n",
    "deltaTable.optimize().executeZOrderBy(\"Department\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10baf96d-fe3b-4c0c-a7f2-6dfa2072aa8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 5: Time Travel with Delta Table\n",
    "1. Retrieve the employees Delta table as it was before the last merge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021d2178-fd4e-4f89-a78f-ffb5f3b203ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmployeeID</th><th>EmployeeName</th><th>Department</th><th>JoiningDate</th><th>Salary</th></tr></thead><tbody><tr><td>101</td><td>John</td><td>HR</td><td>2023-01-10</td><td>50000</td></tr><tr><td>102</td><td>Alice</td><td>Finance</td><td>2023-02-15</td><td>70000</td></tr><tr><td>103</td><td>Mark</td><td>Engineering</td><td>2023-03-20</td><td>85000</td></tr><tr><td>104</td><td>Emma</td><td>Sales</td><td>2023-04-01</td><td>55000</td></tr><tr><td>105</td><td>Liam</td><td>Marketing</td><td>2023-05-12</td><td>60000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "John",
         "HR",
         "2023-01-10",
         "50000"
        ],
        [
         "102",
         "Alice",
         "Finance",
         "2023-02-15",
         "70000"
        ],
        [
         "103",
         "Mark",
         "Engineering",
         "2023-03-20",
         "85000"
        ],
        [
         "104",
         "Emma",
         "Sales",
         "2023-04-01",
         "55000"
        ],
        [
         "105",
         "Liam",
         "Marketing",
         "2023-05-12",
         "60000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmployeeID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmployeeName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JoiningDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge the new data into the existing Delta table\n",
    "deltaTable.alias(\"old\").merge(\n",
    "    new_employees_df.alias(\"new\"),\n",
    "    \"old.EmployeeID = new.EmployeeID\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"salary\": \"new.salary\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"EmployeeID\": \"new.EmployeeID\",\n",
    "    \"EmployeeName\": \"new.EmployeeName\",\n",
    "    \"Department\": \"new.Department\",\n",
    "    \"JoiningDate\": \"new.JoiningDate\",\n",
    "    \"Salary\": \"new.Salary\"\n",
    "}).execute()\n",
    "version_before_merge = deltaTable.history().filter(\"operation = 'MERGE'\").select(\"version\").collect()[0][0]\n",
    "table_path = \"dbfs:/FileStore/streaming/input/employee_data\" \n",
    "df_before_merge = spark.read.format(\"delta\").option(\"versionAsOf\", version_before_merge).load(table_path)\n",
    "display(df_before_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61abff07-9eeb-4998-a056-7bb31fb03e8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Query the table at a specific version to view the older records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee26e3e6-6e5f-428d-b354-91cddfa9c92d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmployeeID</th><th>EmployeeName</th><th>Department</th><th>JoiningDate</th><th>Salary</th></tr></thead><tbody><tr><td>101</td><td>John</td><td>HR</td><td>2023-01-10</td><td>50000</td></tr><tr><td>102</td><td>Alice</td><td>Finance</td><td>2023-02-15</td><td>70000</td></tr><tr><td>103</td><td>Mark</td><td>Engineering</td><td>2023-03-20</td><td>85000</td></tr><tr><td>104</td><td>Emma</td><td>Sales</td><td>2023-04-01</td><td>55000</td></tr><tr><td>105</td><td>Liam</td><td>Marketing</td><td>2023-05-12</td><td>60000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "John",
         "HR",
         "2023-01-10",
         "50000"
        ],
        [
         "102",
         "Alice",
         "Finance",
         "2023-02-15",
         "70000"
        ],
        [
         "103",
         "Mark",
         "Engineering",
         "2023-03-20",
         "85000"
        ],
        [
         "104",
         "Emma",
         "Sales",
         "2023-04-01",
         "55000"
        ],
        [
         "105",
         "Liam",
         "Marketing",
         "2023-05-12",
         "60000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmployeeID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmployeeName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JoiningDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the version you want to query\n",
    "version = 5\n",
    "\n",
    "# Load the Delta table at the specified version\n",
    "df_specific_version = spark.read.format(\"delta\").option(\"versionAsOf\", version).load(\"dbfs:/FileStore/streaming/input/employee_data\")\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_specific_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "330f813c-e389-4e63-8f32-4f80dd70958b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 6: Vacuum Delta Table\n",
    "Use the vacuum operation on the employees Delta table to remove old versions\n",
    "and free up disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc487b16-829c-475b-aedd-68fd55c38895",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/FileStore/streaming/input/employee_data</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/streaming/input/employee_data"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Vacuum the Delta table to remove files older than the default retention period\n",
    "vacuumed: DataFrame = spark.sql(\"VACUUM 'dbfs:/FileStore/streaming/input/employee_data'\")\n",
    "\n",
    "# Display the result of the vacuum operation\n",
    "display(vacuumed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25206413-6143-4255-b1c6-7af8a80e585a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Set the retention period to 7 days and ensure that old files are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0eaac6-2b0e-4eb4-b863-694286743105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/FileStore/streaming/input/employee_data</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/streaming/input/employee_data"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the retention period to 7 days (168 hours) and vacuum the Delta table to remove old files\n",
    "vacuumed: DataFrame = spark.sql(\"VACUUM 'dbfs:/FileStore/streaming/input/employee_data' RETAIN 168 HOURS\")\n",
    "\n",
    "# Display the result of the vacuum operation\n",
    "display(vacuumed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84af8a79-58ad-4794-90d2-2679736b3e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assignment: Structured Streaming and Transformations on Streams\n",
    "Create a folder for streaming CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5988d69-4517-40d5-a7bf-4be66ca5749c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Create a folder for streaming CSV files.\n",
    "2. Set up a structured streaming source to continuously read CSV data from this\n",
    "folder.\n",
    "3. Ensure that the streaming query reads the data continuously in append mode and\n",
    "displays the results in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1213dda1-3cfb-4a00-9cf0-7a982fad1c02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a folder for streaming CSV files\n",
    "dbutils.fs.mkdirs(\"/Workspace/Shared/transaction.csv\")\n",
    "\n",
    "# Define the schema for the CSV files\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"TransactionDate\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Set up a structured streaming source to continuously read CSV data\n",
    "csv_streaming_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)\n",
    "    .csv(\"/Workspace/Shared/transaction.csv\")\n",
    ")\n",
    "\n",
    "query = csv_streaming_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"filtered_transactions\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a16b8aa-89f5-43f3-9649-39a3e387e0fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assign the streaming query to a variable named 'query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf61edef-6f4a-4c4e-ba22-3f368ab2a882",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae3f0f45-e621-4d35-bde3-a21e6ce6a682",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 2: Stream Transformations\n",
    "1. Once the data is streaming in, perform transformations on the incoming data:\n",
    "Add a new column for the TotalAmount ( Quantity * Price ).\n",
    "Filter records where the Quantity is greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99ef3b7-15d5-408b-8afd-e19411505be7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Perform transformations on the incoming streaming data\n",
    "transformed_csv_streaming_df = csv_streaming_df \\\n",
    "    .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
    "    .filter(col(\"Quantity\") > 1)\n",
    "\n",
    "# Assign the transformed stream to a variable for further operations\n",
    "filtered_csv_streaming_df = transformed_csv_streaming_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba5e48ed-718e-4d16-9fc9-81f217c41e47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Write the transformed stream to a memory sink to see the updated results\n",
    "continuously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e1541e1-045e-4175-90b8-3aa56dc9426c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TransactionID</th><th>ProductID</th><th>TransactionDate</th><th>Quantity</th><th>Price</th><th>TotalAmount</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TransactionID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TransactionDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "TotalAmount",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_filtered = (\n",
    "    filtered_csv_streaming_df\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"filtered_csv_data\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "display(spark.sql(\"SELECT * FROM filtered_csv_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65186bbd-5653-4465-ac79-3c1b4eabe227",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop the execution of the stream\n",
    "query_filtered.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e955eee3-1db9-436d-a0c9-d5e252248698",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 3: Aggregations on Streaming Data\n",
    "1. Implement an aggregation on the streaming data:\n",
    "Group the data by ProductID and calculate the total sales for each\n",
    "product (i.e., sum of Quantity * Price for each product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4dd948-4483-4aff-84e7-4e5883fa2949",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ProductID</th><th>TotalSales</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TotalSales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Group the data by ProductID and calculate the total sales for each product\n",
    "product_sales_aggregation = csv_streaming_df.groupBy(\"ProductID\").agg(sum(col(\"Quantity\") * col(\"Price\")).alias(\"TotalSales\"))\n",
    "\n",
    "# Start the streaming query to display the aggregated data\n",
    "query_aggregated = (\n",
    "    product_sales_aggregation\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"aggregated_product_sales\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Display the aggregated data\n",
    "display(spark.sql(\"SELECT * FROM aggregated_product_sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192c802d-a0a4-49e0-be8b-ecbd85340ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_aggregated.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "653d8363-c221-4bd1-a765-8aee82628790",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "204a3526-6e30-4661-83ba-1e57e27e4741",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 4: Writing Streaming Data to File Sinks\n",
    "1. After transforming and aggregating the data, write the streaming results to a\n",
    "Parquet sink.\n",
    "2. Ensure that you configure a checkpoint location to store progress and ensure\n",
    "recovery in case of failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e27848-cddd-48d5-b71a-18f96ac038b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the aggregated stream to a Parquet sink with checkpointing\n",
    "query_aggregated_to_parquet = (\n",
    "    product_sales_aggregation\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", \"/Workspace/Shared/aggregated_dataaggregated_product_sales\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/checkpoints/aggregated_product_sales\")\n",
    "    .outputMode(\"append\")  # Change the output mode to \"append\" or \"update\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9619b300-a2e0-425a-a3e5-f8acb223e835",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 5: Handling Late Data using Watermarks\n",
    "1. Introduce a watermark on the TransactionDate column to handle late data\n",
    "arriving in the stream.\n",
    "2. Set the watermark to 1 day to allow late data within a 24-hour period and\n",
    "discard data that is older."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f54296-aa2b-49fe-9ea3-2bba7485e840",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Convert TransactionDate column to timestamp type\n",
    "transformed_csv_streaming_df = transformed_csv_streaming_df.withColumn(\"TransactionDate\", col(\"TransactionDate\").cast(TimestampType()))\n",
    "\n",
    "# Set watermark on TransactionDate column\n",
    "transformed_csv_streaming_df = transformed_csv_streaming_df.withWatermark(\"TransactionDate\", \"1 day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca40ae4b-3fcd-49de-9d1f-02659240b15e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 6: Streaming from Multiple Sources\n",
    "1. Simulate a scenario where two streams of data are being ingested:\n",
    "Stream 1: Incoming transaction data (same as Task 1).\n",
    "Stream 2: Product information (CSV with columns: ProductID, ProductName,\n",
    "Category).\n",
    "2. Perform a join on the two streams using the ProductID column and display the\n",
    "combined stream results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef07ba2-1325-4aa9-b32e-d95ec03d3393",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1284719369368689>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Step 1: Import necessary libraries and create SparkSession object\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Step 2: Define transaction_stream and product_stream DataFrames\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m transaction_stream \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_format\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_options\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_transaction_data_source\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m product_stream \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_format\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_options\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_product_data_source\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Step 3: Join the transaction_stream with the product_stream on ProductID\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: DataFrameReader.option() missing 1 required positional argument: 'value'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "DataFrameReader.option() missing 1 required positional argument: 'value'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: DataFrameReader.option() missing 1 required positional argument: 'value'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-1284719369368689>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Step 1: Import necessary libraries and create SparkSession object\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Step 2: Define transaction_stream and product_stream DataFrames\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m transaction_stream \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_format\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_options\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_transaction_data_source\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m product_stream \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_format\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_options\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_product_data_source\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Step 3: Join the transaction_stream with the product_stream on ProductID\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "\u001B[0;31mTypeError\u001B[0m: DataFrameReader.option() missing 1 required positional argument: 'value'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join the transaction_stream with the product_stream on ProductID\n",
    "joined_stream = transaction_stream.join(\n",
    "    product_stream,\n",
    "    transaction_stream.ProductID == product_stream.ProductID,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Display the joined stream\n",
    "display(joined_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27e21488-0f20-46cd-b6c6-20d6bc878d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 7: Stopping and Restarting Streaming Queries\n",
    "1. Stop the streaming query and explore the results.\n",
    "2. Restart the query and ensure that it continues from the last processed data by\n",
    "utilizing the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8e3e2d-53ad-4b80-8a14-fce160d0a440",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1284719369368711>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Stop the streaming query\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m joined_stream_query \u001B[38;5;241m=\u001B[39m display(joined_stream, streamName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoinedStreamQuery\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Restart the query with checkpointing to ensure it continues from the last processed data\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m joined_stream_query_restart \u001B[38;5;241m=\u001B[39m joined_stream\u001B[38;5;241m.\u001B[39mwriteStream \\\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoinedStreamQuery\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/path/to/checkpoint/dir\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'joined_stream' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'joined_stream' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'joined_stream' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-1284719369368711>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Stop the streaming query\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m joined_stream_query \u001B[38;5;241m=\u001B[39m display(joined_stream, streamName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoinedStreamQuery\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mawaitTermination()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Restart the query with checkpointing to ensure it continues from the last processed data\u001B[39;00m\n\u001B[1;32m      5\u001B[0m joined_stream_query_restart \u001B[38;5;241m=\u001B[39m joined_stream\u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoinedStreamQuery\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/path/to/checkpoint/dir\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
        "\u001B[0;31mNameError\u001B[0m: name 'joined_stream' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop the streaming query\n",
    "joined_stream_query = display(joined_stream, streamName=\"joinedStreamQuery\").awaitTermination()\n",
    "\n",
    "# Restart the query with checkpointing to ensure it continues from the last processed data\n",
    "joined_stream_query_restart = joined_stream.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"joinedStreamQuery\") \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06349e08-d3bf-4702-865f-adcc1c760ac0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assignment: Creating a Complete ETL Pipeline using Delta Live Tables\n",
    "(DLT)\n",
    "Task 1: Create an ETL Pipeline using DLT (Python)\n",
    "1. Create a Delta Live Table pipeline using PySpark to perform the following:\n",
    "Read the source data from a CSV or Parquet file.\n",
    "Transform the data by performing the following:\n",
    "Add a new column for TotalAmount which is the result of\n",
    "multiplying Quantity by Price .\n",
    "Filter records where the Quantity is greater than 1.\n",
    "Load the transformed data into a Delta table.\n",
    "\n",
    "2. Ensure the pipeline is repeatable and can handle incremental loads by re-running with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5495580f-d51f-4f31-a73d-281e71cdc6db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_csv_path = 'file:/Workspace/Shared/orders.csv'\n",
    "dbutils.fs.cp(orders_csv_path, \"dbfs:/Workspace/Shared/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d11fb32-88aa-4b5c-a31a-e3e0d1d47df9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>orders_final</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>OrderID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>OrderDate</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>CustomerID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Product</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Quantity</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Price</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>TotalAmount</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=3929522205148641#joblist/pipelines/create?initialSource=%2FUsers%2Fazuser2110_mml.local%40techademy.com%2FSept%2017%20exe&redirectNotebookId=1284719369368105\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Read data from a CSV source\n",
    "@dlt.table\n",
    "def orders_raw():\n",
    "    return spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/Workspace/Shared/orders.csv\")\n",
    "\n",
    "# Transform data (add TotalAmount and filter Quantity > 1)\n",
    "@dlt.table\n",
    "def orders_transformed():\n",
    "    df = dlt.read(\"orders_raw\")\n",
    "    df = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
    "    return df.filter(col(\"Quantity\") > 1)\n",
    "\n",
    "# Load the transformed data into a Delta table\n",
    "@dlt.table\n",
    "def orders_final():\n",
    "    dlt.read(\"orders_transformed\").write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Workspace/Sharedorders_final\")\n",
    "    return dlt.read(\"orders_transformed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f38c8c5-3de8-4559-abfc-a183a7f2fbec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 2: Create an ETL Pipeline using DLT (SQL)\n",
    "1. Create a similar Delta Live Table pipeline using SQL:\n",
    "Use SQL to read the source data, perform the same transformations (as\n",
    "above), and write the data into a Delta table.\n",
    "\n",
    "Ensure the pipeline can process incremental data without losing records\n",
    "or creating duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c969ed65-5c9a-4049-a232-5174d824011d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------+--------+-----+\n|    _c0|       _c1|       _c2|    _c3|     _c4|  _c5|\n+-------+----------+----------+-------+--------+-----+\n|OrderID| OrderDate|CustomerID|Product|Quantity|Price|\n|    101|2024-01-01|      C001| Laptop|       2| 1000|\n|    102|2024-01-02|      C002|  Phone|       1|  500|\n|    103|2024-01-03|      C003| Tablet|       3|  300|\n|    104|2024-01-04|      C004|Monitor|       1|  150|\n|    105|2024-01-05|      C005|  Mouse|       5|   20|\n+-------+----------+----------+-------+--------+-----+\n\n+-------+----------+----------+--------+--------+-----+\n|    _c0|       _c1|       _c2|     _c3|     _c4|  _c5|\n+-------+----------+----------+--------+--------+-----+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n|    101|2024-01-01|      C001|  Laptop|       2| 1000|\n|    102|2024-01-02|      C002|   Phone|       1|  500|\n|    103|2024-01-03|      C003|  Tablet|       3|  300|\n|    104|2024-01-04|      C004| Monitor|       1|  150|\n|    105|2024-01-05|      C005|   Mouse|       5|   20|\n|    106|2024-01-12|      C006|Keyboard|       3|   50|\n+-------+----------+----------+--------+--------+-----+\n\n+---+----------+----+------+---+----+------+\n|_c0|       _c1| _c2|   _c3|_c4| _c5| Price|\n+---+----------+----+------+---+----+------+\n|101|2024-01-01|C001|Laptop|  2|1000|1100.0|\n+---+----------+----+------+---+----+------+\n\n+---+----------+----+------+---+----+------+\n|_c0|       _c1| _c2|   _c3|_c4| _c5| Price|\n+---+----------+----+------+---+----+------+\n|101|2024-01-01|C001|Laptop|  2|1000|1100.0|\n+---+----------+----+------+---+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "df = spark.read.format(\"csv\").load(\"dbfs:/Workspace/Shared/orders.csv\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/assignment17sep/delta/orders\")\n",
    "\n",
    "# Read data from Delta Table\n",
    "df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/assignment17sep/delta/orders\")\n",
    "df.show()\n",
    "\n",
    "# Insert new record\n",
    "df = df.union(spark.createDataFrame([(106, \"2024-01-12\", \"C006\", \"Keyboard\", 3, 50)], [\"OrderID\", \"OrderDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]))\n",
    "df.show()\n",
    "\n",
    "# Update prices (increase price by 10%)\n",
    "df = df.filter(col(\"_c3\") == \"Laptop\").withColumn(\"Price\", col(\"_c5\") * 1.1)\n",
    "df.show()\n",
    "\n",
    "# Delete rows where Quantity < 2\n",
    "df = df.filter(col(\"_c4\") >= 2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b0247f-936e-48e1-bd75-a756758a028a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 3: Perform Read, Write, Update, and Delete Operations on Delta Table\n",
    "(SQL + PySpark)\n",
    "1. Read the data from the Delta table created in Task 1 and Task 2.\n",
    "2. Update the table by changing the price of a product (e.g., increase the price\n",
    "of laptops by 10%).\n",
    "3. Delete rows from the Delta table where the quantity is less than 2.\n",
    "4. Insert a new record into the Delta table using PySpark or SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d176c7f-67f3-441d-880f-74e7142aafe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-458381364531283>, line 9\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCREATE TABLE IF NOT EXISTS delta_orders_table USING DELTA LOCATION \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/assignment17sep/delta/orders_final\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Update prices (increase laptops by 10%)\u001B[39;00m\n",
       "\u001B[0;32m----> 9\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUPDATE delta_orders_table SET Price = Price * 1.1 WHERE Product = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLaptop\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Delete rows where Quantity < 2\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDELETE FROM delta_orders_table WHERE Quantity < 2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/dlt/overrides.py:128\u001B[0m, in \u001B[0;36m_override_spark_functions.<locals>._dlt_sql_fn_with_analysis_api\u001B[0;34m(self, stmt)\u001B[0m\n",
       "\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_dlt_sql_fn_with_analysis_api\u001B[39m(\u001B[38;5;28mself\u001B[39m, stmt):\n",
       "\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _sql_with_analysis_api(stmt)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/dlt/overrides.py:97\u001B[0m, in \u001B[0;36m_sql_with_analysis_api\u001B[0;34m(stmt)\u001B[0m\n",
       "\u001B[1;32m     92\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     93\u001B[0m \u001B[38;5;124;03mSame as [[_sql]], except the SQL query is analyzed using the analysis API.\u001B[39;00m\n",
       "\u001B[1;32m     94\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     95\u001B[0m instance \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39minstance\n",
       "\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\n",
       "\u001B[0;32m---> 97\u001B[0m     instance\u001B[38;5;241m.\u001B[39mget_spark_context()\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mcom\u001B[38;5;241m.\u001B[39mdatabricks\u001B[38;5;241m.\u001B[39mpipelines\u001B[38;5;241m.\u001B[39mSQLPipelineHelper\u001B[38;5;241m.\u001B[39msqlWithAnalysisApi(\n",
       "\u001B[1;32m     98\u001B[0m         instance\u001B[38;5;241m.\u001B[39mget_scala_pipeline(), stmt\n",
       "\u001B[1;32m     99\u001B[0m     ),\n",
       "\u001B[1;32m    100\u001B[0m     instance\u001B[38;5;241m.\u001B[39mget_spark_context()\n",
       "\u001B[1;32m    101\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `Price` cannot be resolved.  SQLSTATE: 42703; line 1 pos 30;\n",
       "'DeltaUpdateTable ['Price], [('Price * 1.1)], ('Product = Laptop)\n",
       "+- SubqueryAlias spark_catalog.default.delta_orders_table\n",
       "   +- Relation spark_catalog.default.delta_orders_table[] parquet\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `Price` cannot be resolved.  SQLSTATE: 42703; line 1 pos 30;\n'DeltaUpdateTable ['Price], [('Price * 1.1)], ('Product = Laptop)\n+- SubqueryAlias spark_catalog.default.delta_orders_table\n   +- Relation spark_catalog.default.delta_orders_table[] parquet\n"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `Price` cannot be resolved.  SQLSTATE: 42703"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42703",
        "stackTrace": null,
        "startIndex": 30,
        "stopIndex": 48
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-458381364531283>, line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCREATE TABLE IF NOT EXISTS delta_orders_table USING DELTA LOCATION \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/assignment17sep/delta/orders_final\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Update prices (increase laptops by 10%)\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUPDATE delta_orders_table SET Price = Price * 1.1 WHERE Product = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLaptop\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Delete rows where Quantity < 2\u001B[39;00m\n\u001B[1;32m     12\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDELETE FROM delta_orders_table WHERE Quantity < 2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/spark/python/dlt/overrides.py:128\u001B[0m, in \u001B[0;36m_override_spark_functions.<locals>._dlt_sql_fn_with_analysis_api\u001B[0;34m(self, stmt)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_dlt_sql_fn_with_analysis_api\u001B[39m(\u001B[38;5;28mself\u001B[39m, stmt):\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _sql_with_analysis_api(stmt)\n",
        "File \u001B[0;32m/databricks/spark/python/dlt/overrides.py:97\u001B[0m, in \u001B[0;36m_sql_with_analysis_api\u001B[0;34m(stmt)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;124;03mSame as [[_sql]], except the SQL query is analyzed using the analysis API.\u001B[39;00m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     95\u001B[0m instance \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39minstance\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\n\u001B[0;32m---> 97\u001B[0m     instance\u001B[38;5;241m.\u001B[39mget_spark_context()\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mcom\u001B[38;5;241m.\u001B[39mdatabricks\u001B[38;5;241m.\u001B[39mpipelines\u001B[38;5;241m.\u001B[39mSQLPipelineHelper\u001B[38;5;241m.\u001B[39msqlWithAnalysisApi(\n\u001B[1;32m     98\u001B[0m         instance\u001B[38;5;241m.\u001B[39mget_scala_pipeline(), stmt\n\u001B[1;32m     99\u001B[0m     ),\n\u001B[1;32m    100\u001B[0m     instance\u001B[38;5;241m.\u001B[39mget_spark_context()\n\u001B[1;32m    101\u001B[0m )\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `Price` cannot be resolved.  SQLSTATE: 42703; line 1 pos 30;\n'DeltaUpdateTable ['Price], [('Price * 1.1)], ('Product = Laptop)\n+- SubqueryAlias spark_catalog.default.delta_orders_table\n   +- Relation spark_catalog.default.delta_orders_table[] parquet\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SQL\n",
    "\n",
    "#SQL\n",
    "\n",
    "# Read data as Delta Table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_orders_table USING DELTA LOCATION 'dbfs:/FileStore/assignment17sep/delta/orders_final'\")\n",
    "\n",
    "# Update prices (increase laptops by 10%)\n",
    "spark.sql(\"UPDATE delta_orders_table SET Price = Price * 1.1 WHERE Product = 'Laptop'\").show()\n",
    "\n",
    "# Delete rows where Quantity < 2\n",
    "spark.sql(\"DELETE FROM delta_orders_table WHERE Quantity < 2\").show()\n",
    "\n",
    "# Insert new record\n",
    "spark.sql(\"INSERT INTO delta_orders_table (OrderID, OrderDate, CustomerID, Product, Quantity, Price) VALUES (106, '2024-01-12', 'C006', 'Keyboard', 3, 50)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9a52d54-5b0f-4875-af88-18576d8526cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 4: Merge Data (Slowly Changing Dimension - SCD Type 2)\n",
    "1. Create a new dataset representing updated orders with new prices and products.\n",
    "Implement a MERGE operation to simulate a Slowly Changing Dimension Type 2\n",
    "(SCD2) scenario. Ensure that:\n",
    "The Quantity , Price , and TotalAmount columns are updated if there is\n",
    "a match on OrderID .\n",
    "If no match is found, insert the new record into the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fceabbbd-4d65-48d4-89ea-be4eb2685d36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging new data into Delta table...\nNew data merged successfully!\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (101, '2024-01-10', 'C001', 'Laptop', 2, 1200), \n",
    "    (106, '2024-01-12', 'C006', 'Keyboard', 3, 50)\n",
    "    ]\n",
    "    \n",
    "schema = [\"OrderID\", \"OrderDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n",
    "\n",
    "\n",
    "new_orders_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "new_orders_df.createOrReplaceTempView(\"new_orders_data\")\n",
    "\n",
    "print(\"Merging new data into Delta table...\")\n",
    "\n",
    "orders_df = spark.read.csv(\"dbfs:/Workspace/Shared/orders.csv\", header=True, inferSchema=True)\n",
    "orders_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/assignment17sep/delta/orders1\")\n",
    "\n",
    "\n",
    "dbfs_path = 'dbfs:/FileStore/assignment17sep/delta/orders1'\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO delta.`{dbfs_path}` AS target\n",
    "USING new_orders_data AS source\n",
    "ON target.OrderID = source.OrderID\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.Quantity = source.Quantity, target.Price = source.Price\n",
    "WHEN NOT MATCHED THEN INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price) \n",
    "VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n",
    "\"\"\")\n",
    "\n",
    "print(\"New data merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d8b59f8-0d2c-4b42-942a-684f6eac1eea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 5: Explore Delta Table Internals\n",
    "1. Inspect the Delta table's transaction logs and explore the metadata using SQL\n",
    "queries:\n",
    "Display the history of changes to the Delta table using the DESCRIBE\n",
    "HISTORY command.\n",
    "Check the file size and modification times using DESCRIBE DETAIL ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c1b207-b183-418a-aaff-fbbeba2ceb03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>clusteringColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th><th>tableFeatures</th><th>statistics</th></tr></thead><tbody><tr><td>delta</td><td>aae9b655-1e4a-43b4-baef-e17c900ef700</td><td>null</td><td>null</td><td>dbfs:/FileStore/assignment17sep/delta/orders</td><td>2024-09-17T09:18:52.338Z</td><td>2024-09-17T09:18:53Z</td><td>List()</td><td>List()</td><td>1</td><td>1559</td><td>Map(delta.enableDeletionVectors -> true)</td><td>3</td><td>7</td><td>List(deletionVectors)</td><td>Map(numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "delta",
         "aae9b655-1e4a-43b4-baef-e17c900ef700",
         null,
         null,
         "dbfs:/FileStore/assignment17sep/delta/orders",
         "2024-09-17T09:18:52.338Z",
         "2024-09-17T09:18:53Z",
         [],
         [],
         1,
         1559,
         {
          "delta.enableDeletionVectors": "true"
         },
         3,
         7,
         [
          "deletionVectors"
         ],
         {
          "numDeletionVectors": 0,
          "numRowsDeletedByDeletionVectors": 0
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 64
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "format",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "createdAt",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "lastModified",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "partitionColumns",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "clusteringColumns",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "numFiles",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sizeInBytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "minReaderVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "minWriterVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "tableFeatures",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "statistics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"long\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- View the history of changes\n",
    "DESCRIBE HISTORY delta.`dbfs:/FileStore/assignment17sep/delta/orders`;\n",
    "\n",
    "-- View the detailed metadata\n",
    "DESCRIBE DETAIL delta.`dbfs:/FileStore/assignment17sep/delta/orders`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d69a2cc-34a3-47c7-833c-6df50c5400fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 6: Time Travel in Delta Tables\n",
    "1. Use time travel to query the Delta table as it existed at a previous point in\n",
    "time.\n",
    "Query the table as it existed before the last merge operation.\n",
    "Demonstrate time travel by using both the version of the table and the\n",
    "timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d3d129-5018-430a-b523-7aa8c7aa580e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>OrderID</td><td>OrderDate</td><td>CustomerID</td><td>Product</td><td>Quantity</td><td>Price</td></tr><tr><td>101</td><td>2024-01-01</td><td>C001</td><td>Laptop</td><td>2</td><td>1000</td></tr><tr><td>102</td><td>2024-01-02</td><td>C002</td><td>Phone</td><td>1</td><td>500</td></tr><tr><td>103</td><td>2024-01-03</td><td>C003</td><td>Tablet</td><td>3</td><td>300</td></tr><tr><td>104</td><td>2024-01-04</td><td>C004</td><td>Monitor</td><td>1</td><td>150</td></tr><tr><td>105</td><td>2024-01-05</td><td>C005</td><td>Mouse</td><td>5</td><td>20</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "OrderID",
         "OrderDate",
         "CustomerID",
         "Product",
         "Quantity",
         "Price"
        ],
        [
         "101",
         "2024-01-01",
         "C001",
         "Laptop",
         "2",
         "1000"
        ],
        [
         "102",
         "2024-01-02",
         "C002",
         "Phone",
         "1",
         "500"
        ],
        [
         "103",
         "2024-01-03",
         "C003",
         "Tablet",
         "3",
         "300"
        ],
        [
         "104",
         "2024-01-04",
         "C004",
         "Monitor",
         "1",
         "150"
        ],
        [
         "105",
         "2024-01-05",
         "C005",
         "Mouse",
         "5",
         "20"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 66
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c4",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c5",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Query the table before the last merge\n",
    "SELECT * FROM delta.`dbfs:/FileStore/assignment17sep/delta/orders` VERSION AS OF 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7d81cef-a3de-45e2-9ca3-16c698f23e40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 7: Optimize Delta Table\n",
    "1. Optimize the Delta table for faster queries using Z-Ordering.\n",
    "Optimize the table on the Product column to reduce I/O and improve\n",
    "query performance.\n",
    "2. Use vacuum to remove any old files that are no longer necessary after the\n",
    "optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467d5e33-0d93-49c2-82f1-79956d8e8aab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize the table for faster queries with Z-ordering\n",
    "spark.sql(\"OPTIMIZE delta.`dbfs:/FileStore/assignment17sep/delta/orders`\")\n",
    "\n",
    "# Vacuum the table to remove old files\n",
    "spark.sql(\"VACUUM delta.`dbfs:/FileStore/assignment17sep/delta/orders` RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28ee4c7d-8343-4f3c-953b-1ba4b2db3108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Task 8: Converting Parquet Files to Delta Format\n",
    "1. You are provided with Parquet files containing historical order data. Convert\n",
    "these files into a Delta table format using either PySpark or SQL.\n",
    "Perform a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aba4b787-2080-47bc-9d64-604b78234ef6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TotalOrders</th></tr></thead><tbody><tr><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TotalOrders",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Step 1: Create a DataFrame with the specified schema\n",
    "data = [\n",
    "    (101, '2024-01-10', 'C001', 'Laptop', 2, 1200), \n",
    "    (106, '2024-01-12', 'C006', 'Keyboard', 3, 50)\n",
    "]\n",
    "schema = [\"OrderID\", \"OrderDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n",
    "orders_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Step 2: Write the DataFrame to a Parquet file\n",
    "parquet_path = \"dbfs:/Workspace/Shared/historical_orders.parquet\"\n",
    "orders_df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "# Step 3: Read the Parquet file into a DataFrame\n",
    "historical_orders_df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Step 4: Convert the DataFrame into a Delta table\n",
    "delta_path = \"dbfs:/FileStore/historical_orders/delta\"\n",
    "historical_orders_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Step 5: Create a temporary view and perform SQL queries\n",
    "spark.read.format(\"delta\").load(delta_path).createOrReplaceTempView(\"historical_orders_delta\")\n",
    "display(spark.sql(\"SELECT COUNT(*) AS TotalOrders FROM historical_orders_delta\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 458381364531339,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Sept 17 exe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
