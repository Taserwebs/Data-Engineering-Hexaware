{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c016a4a779c04721be55ca865ab1b876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "age",
              "salary"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Filter By:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_9333018ea6a84aa19f848985300429e3",
            "style": "IPY_MODEL_d09d061d7d244e01a7905e57802b977c"
          }
        },
        "9333018ea6a84aa19f848985300429e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d09d061d7d244e01a7905e57802b977c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b7c4f3c94a64ed4b212552318cfa3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": false,
            "description": "Threshold:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ee96ea39b0174de083241263abed3ead",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 5,
            "style": "IPY_MODEL_9a869f8dd3704664a7703728f8ef6369",
            "value": 30
          }
        },
        "ee96ea39b0174de083241263abed3ead": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a869f8dd3704664a7703728f8ef6369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "667e7ca523d643118f3fac71f5003de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Apply Filter",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_69acc13ffc6e4b3481b1ab38f8b457ea",
            "style": "IPY_MODEL_6e48ea452abd445c8657b7c1092a2256",
            "tooltip": ""
          }
        },
        "69acc13ffc6e4b3481b1ab38f8b457ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e48ea452abd445c8657b7c1092a2256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0f4475c8582140898fb757aa06e1ed2b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_9355276f219c482393407d6e60c210f1",
            "msg_id": "",
            "outputs": []
          }
        },
        "9355276f219c482393407d6e60c210f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndsX_NnBjkrz",
        "outputId": "6699aedd-82bf-44e6-e733-2402fce8eb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=248c3d3e122b8ea5998df8e165f7bfb5ac0ca1558ba421c2d96160a80ea7c494\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"PySpark DataFrame Example\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "# Sample data representing employees\n",
        "\n",
        "data = [\n",
        "\n",
        "    (\"John Doe\", \"Engineering\", 75000),\n",
        "\n",
        "    (\"Jane Smith\", \"Marketing\", 60000),\n",
        "\n",
        "    (\"Sam Brown\", \"Engineering\", 80000),\n",
        "\n",
        "    (\"Emily Davis\", \"HR\", 50000),\n",
        "\n",
        "    (\"Michael Johnson\", \"Marketing\", 70000),\n",
        "\n",
        "]\n",
        "\n",
        "# Define schema for DataFrame\n",
        "\n",
        "columns = [\"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "# Create DataFrame\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "\n",
        "df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7WZRXankHOe",
        "outputId": "4415f61d-109e-458f-8775-59e3dcd0605f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+------+\n",
            "|           Name| Department|Salary|\n",
            "+---------------+-----------+------+\n",
            "|       John Doe|Engineering| 75000|\n",
            "|     Jane Smith|  Marketing| 60000|\n",
            "|      Sam Brown|Engineering| 80000|\n",
            "|    Emily Davis|         HR| 50000|\n",
            "|Michael Johnson|  Marketing| 70000|\n",
            "+---------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter: Select employees with a salary greater than 65,000\n",
        "high_salary_df = df.filter(col(\"Salary\") > 65000)\n",
        "print(\"Employees with Salary > 65,000:\")\n",
        "# Show the filtered DataFrame\n",
        "high_salary_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehpWz3__qRR2",
        "outputId": "6fcced8c-b856-484e-8b76-61616eaa1d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees with Salary > 65,000:\n",
            "+---------------+-----------+------+\n",
            "|           Name| Department|Salary|\n",
            "+---------------+-----------+------+\n",
            "|       John Doe|Engineering| 75000|\n",
            "|      Sam Brown|Engineering| 80000|\n",
            "|Michael Johnson|  Marketing| 70000|\n",
            "+---------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Department and Calculate the average salary\n",
        "avg_salary_by_df = df.groupBy(\"Department\").avg(\"Salary\")\n",
        "print(\"Average Salary by Department:\")\n",
        "# Show the average salary by department\n",
        "avg_salary_by_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfJ6ZVpArCuR",
        "outputId": "4d644f7f-f611-4f14-d523-9dfd7a9d0193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Salary by Department:\n",
            "+-----------+-----------+\n",
            "| Department|avg(Salary)|\n",
            "+-----------+-----------+\n",
            "|Engineering|    77500.0|\n",
            "|  Marketing|    65000.0|\n",
            "|         HR|    50000.0|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Customer Transaction Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data for customers\n",
        "customers = [\n",
        "    (1, \"Ravi\", \"Mumbai\"),\n",
        "    (2, \"Priya\", \"Delhi\"),\n",
        "    (3, \"Vijay\", \"Bangalore\"),\n",
        "    (4, \"Anita\", \"Chennai\"),\n",
        "    (5, \"Raj\", \"Hyderabad\"),\n",
        "]\n",
        "\n",
        "# Sample data for transactions\n",
        "transactions = [\n",
        "    (1, 1, 10000.50),\n",
        "    (2, 2, 20000.75),\n",
        "    (3, 1, 15000.25),\n",
        "    (4, 3, 30000.00),\n",
        "    (5, 2, 40000.50),\n",
        "    (6, 4, 25000.00),\n",
        "    (7, 5, 18000.75),\n",
        "    (8, 1, 5000.00),\n",
        "]\n",
        "\n",
        "# Define schema for DataFrames\n",
        "customer_columns = [\"customer_id\", \"Name\", \"city\"]\n",
        "transaction_columns = [\"Transaction_id\", \"customer_id\", \"Amount\"]  # Changed CustomerId to customer_id\n",
        "\n",
        "# Create DataFrames\n",
        "customer_df = spark.createDataFrame(customers, schema=customer_columns)\n",
        "transactions_df = spark.createDataFrame(transactions, schema=transaction_columns)\n",
        "\n",
        "# Show the DataFrames\n",
        "print(\"Customers DataFrame:\")\n",
        "customer_df.show()\n",
        "\n",
        "print(\"Transactions DataFrame:\")\n",
        "transactions_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DKP07IxBMfG",
        "outputId": "e5a27045-3226-43f8-9cb1-f01302d9cdf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers DataFrame:\n",
            "+-----------+-----+---------+\n",
            "|customer_id| Name|     city|\n",
            "+-----------+-----+---------+\n",
            "|          1| Ravi|   Mumbai|\n",
            "|          2|Priya|    Delhi|\n",
            "|          3|Vijay|Bangalore|\n",
            "|          4|Anita|  Chennai|\n",
            "|          5|  Raj|Hyderabad|\n",
            "+-----------+-----+---------+\n",
            "\n",
            "Transactions DataFrame:\n",
            "+--------------+-----------+--------+\n",
            "|Transaction_id|customer_id|  Amount|\n",
            "+--------------+-----------+--------+\n",
            "|             1|          1| 10000.5|\n",
            "|             2|          2|20000.75|\n",
            "|             3|          1|15000.25|\n",
            "|             4|          3| 30000.0|\n",
            "|             5|          2| 40000.5|\n",
            "|             6|          4| 25000.0|\n",
            "|             7|          5|18000.75|\n",
            "|             8|          1|  5000.0|\n",
            "+--------------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the DataFrames on customer_id\n",
        "customer_transactions_df = customer_df.join(transactions_df, on=\"customer_id\")\n",
        "print(\"Customer Transactions DataFrame:\")\n",
        "customer_transactions_df.show()\n",
        "\n",
        "# Calculate the total amount spent by each customer\n",
        "total_spent_df = customer_transactions_df.groupBy(\"Name\").sum(\"Amount\").withColumnRenamed(\"sum(Amount)\", \"TotalSpent\")\n",
        "print(\"Total Amount Spent by Each Customer:\")\n",
        "total_spent_df.show()\n",
        "\n",
        "# Find customers who have spent more than 30000\n",
        "big_spender_df = total_spent_df.filter(col(\"TotalSpent\") > 30000)\n",
        "print(\"Customers who have spent more than 30000:\")\n",
        "big_spender_df.show()\n",
        "\n",
        "# Count the number of transactions per customer\n",
        "transaction_count_df = customer_transactions_df.groupBy(\"Name\").count().withColumnRenamed(\"count\", \"TransactionCount\")\n",
        "print(\"Number of Transactions per Customer:\")\n",
        "transaction_count_df.show()\n",
        "\n",
        "# Sort customers by total amount spent in descending order\n",
        "sorted_spenders_df = total_spent_df.orderBy(col(\"TotalSpent\").desc())\n",
        "print(\"Customers Sorted by Total Amount Spent:\")\n",
        "sorted_spenders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeTrqbPHJCyy",
        "outputId": "9804dcc5-f81d-4337-e04d-2587f50f99a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Transactions DataFrame:\n",
            "+-----------+-----+---------+--------------+--------+\n",
            "|customer_id| Name|     city|Transaction_id|  Amount|\n",
            "+-----------+-----+---------+--------------+--------+\n",
            "|          1| Ravi|   Mumbai|             1| 10000.5|\n",
            "|          1| Ravi|   Mumbai|             3|15000.25|\n",
            "|          1| Ravi|   Mumbai|             8|  5000.0|\n",
            "|          2|Priya|    Delhi|             2|20000.75|\n",
            "|          2|Priya|    Delhi|             5| 40000.5|\n",
            "|          3|Vijay|Bangalore|             4| 30000.0|\n",
            "|          4|Anita|  Chennai|             6| 25000.0|\n",
            "|          5|  Raj|Hyderabad|             7|18000.75|\n",
            "+-----------+-----+---------+--------------+--------+\n",
            "\n",
            "Total Amount Spent by Each Customer:\n",
            "+-----+----------+\n",
            "| Name|TotalSpent|\n",
            "+-----+----------+\n",
            "| Ravi|  30000.75|\n",
            "|Priya|  60001.25|\n",
            "|Vijay|   30000.0|\n",
            "|Anita|   25000.0|\n",
            "|  Raj|  18000.75|\n",
            "+-----+----------+\n",
            "\n",
            "Customers who have spent more than 30000:\n",
            "+-----+----------+\n",
            "| Name|TotalSpent|\n",
            "+-----+----------+\n",
            "| Ravi|  30000.75|\n",
            "|Priya|  60001.25|\n",
            "+-----+----------+\n",
            "\n",
            "Number of Transactions per Customer:\n",
            "+-----+----------------+\n",
            "| Name|TransactionCount|\n",
            "+-----+----------------+\n",
            "| Ravi|               3|\n",
            "|Priya|               2|\n",
            "|Vijay|               1|\n",
            "|Anita|               1|\n",
            "|  Raj|               1|\n",
            "+-----+----------------+\n",
            "\n",
            "Customers Sorted by Total Amount Spent:\n",
            "+-----+----------+\n",
            "| Name|TotalSpent|\n",
            "+-----+----------+\n",
            "|Priya|  60001.25|\n",
            "| Ravi|  30000.75|\n",
            "|Vijay|   30000.0|\n",
            "|Anita|   25000.0|\n",
            "|  Raj|  18000.75|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjkmvufhZOGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### **Exercise: Product Sales Analysis**\n",
        "\n",
        "# #### **Step 1: Create DataFrames**\n",
        "\n",
        "# You will create two DataFrames: one for products and another for sales transactions. Then, you’ll perform operations like joining these DataFrames and analyzing the data.\n",
        "\n",
        "# ```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Product Sales Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data for products\n",
        "products = [\n",
        "    (1, \"Laptop\", \"Electronics\", 50000),\n",
        "    (2, \"Smartphone\", \"Electronics\", 30000),\n",
        "    (3, \"Table\", \"Furniture\", 15000),\n",
        "    (4, \"Chair\", \"Furniture\", 5000),\n",
        "    (5, \"Headphones\", \"Electronics\", 2000),\n",
        "]\n",
        "\n",
        "# Sample data for sales transactions\n",
        "sales = [\n",
        "    (1, 1, 2),\n",
        "    (2, 2, 1),\n",
        "    (3, 3, 3),\n",
        "    (4, 1, 1),\n",
        "    (5, 4, 5),\n",
        "    (6, 2, 2),\n",
        "    (7, 5, 10),\n",
        "    (8, 3, 1),\n",
        "]\n",
        "\n",
        "# Define schema for DataFrames\n",
        "product_columns = [\"ProductID\", \"ProductName\", \"Category\", \"Price\"]\n",
        "sales_columns = [\"SaleID\", \"ProductID\", \"Quantity\"]\n",
        "\n",
        "# Create DataFrames\n",
        "product_df = spark.createDataFrame(products, schema=product_columns)\n",
        "sales_df = spark.createDataFrame(sales, schema=sales_columns)\n",
        "\n",
        "# Show the DataFrames\n",
        "print(\"Products DataFrame:\")\n",
        "product_df.show()\n",
        "\n",
        "print(\"Sales DataFrame:\")\n",
        "sales_df.show()\n",
        "# ```\n",
        "\n",
        "# #### **Step 2: Perform the Following Tasks**\n",
        "\n",
        "# 1. **Join the DataFrames:**\n",
        "#    - Join the `product_df` and `sales_df` DataFrames on `ProductID` to create a combined DataFrame with product and sales data.\n",
        "product_and_sales_df = product_df.join(sales_df, on=\"ProductID\")\n",
        "print(\"Product and Sales DataFrame:\")\n",
        "product_and_sales_df.show()\n",
        "\n",
        "# 2. **Calculate Total Sales Value:**\n",
        "#    - For each product, calculate the total sales value by multiplying the price by the quantity sold.\n",
        "total_sales_df = product_and_sales_df.withColumn(\"TotalSales\", col(\"Price\") * col(\"Quantity\"))\n",
        "print(\"Total Sales DataFrame:\")\n",
        "total_sales_df.show()\n",
        "\n",
        "# 3. **Find the Total Sales for Each Product Category:**\n",
        "#    - Group the data by the `Category` column and calculate the total sales value for each product category.\n",
        "total_sales_by_category_df = total_sales_df.groupBy(\"Category\").sum(\"TotalSales\").withColumnRenamed(\"sum(TotalSales)\", \"TotalSalesCategory\")\n",
        "print(\"Total Sales by Product Category:\")\n",
        "total_sales_by_category_df.show()\n",
        "\n",
        "# 4. **Identify the Top-Selling Product:**\n",
        "#    - Find the product that generated the highest total sales value.\n",
        "top_selling_product_df = total_sales_df.groupBy(\"ProductName\").sum(\"TotalSales\").withColumnRenamed(\"sum(TotalSales)\", \"TotalSalesProduct\")\n",
        "top_selling_product_df\n",
        "\n",
        "# 5. **Sort the Products by Total Sales Value:**\n",
        "#    - Sort the products by total sales value in descending order.\n",
        "sorted_products_df = top_selling_product_df.orderBy(col(\"TotalSalesProduct\").desc())\n",
        "print(\"Sorted Products by Total Sales Value:\")\n",
        "sorted_products_df.show()\n",
        "\n",
        "# 6. **Count the Number of Sales for Each Product:**\n",
        "#    - Count the number of sales transactions for each product.\n",
        "sales_count_df = total_sales_df.groupBy(\"ProductName\").count().withColumnRenamed(\"count\", \"SalesCount\")\n",
        "print(\"Number of Sales for Each Product:\")\n",
        "sales_count_df.show()\n",
        "\n",
        "# 7. **Filter the Products with Total Sales Value Greater Than ₹50,000:**\n",
        "#    - Filter out the products that have a total sales value greater than ₹50,000.\n",
        "high_value_products_df = total_sales_df.filter(col(\"TotalSales\") > 50000)\n",
        "print(\"Products with Total Sales Value Greater Than ₹50,000:\")\n",
        "high_value_products_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2VRd67CRdMu",
        "outputId": "f5409ca8-720c-4ef2-fb2e-851defa196c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products DataFrame:\n",
            "+---------+-----------+-----------+-----+\n",
            "|ProductID|ProductName|   Category|Price|\n",
            "+---------+-----------+-----------+-----+\n",
            "|        1|     Laptop|Electronics|50000|\n",
            "|        2| Smartphone|Electronics|30000|\n",
            "|        3|      Table|  Furniture|15000|\n",
            "|        4|      Chair|  Furniture| 5000|\n",
            "|        5| Headphones|Electronics| 2000|\n",
            "+---------+-----------+-----------+-----+\n",
            "\n",
            "Sales DataFrame:\n",
            "+------+---------+--------+\n",
            "|SaleID|ProductID|Quantity|\n",
            "+------+---------+--------+\n",
            "|     1|        1|       2|\n",
            "|     2|        2|       1|\n",
            "|     3|        3|       3|\n",
            "|     4|        1|       1|\n",
            "|     5|        4|       5|\n",
            "|     6|        2|       2|\n",
            "|     7|        5|      10|\n",
            "|     8|        3|       1|\n",
            "+------+---------+--------+\n",
            "\n",
            "Product and Sales DataFrame:\n",
            "+---------+-----------+-----------+-----+------+--------+\n",
            "|ProductID|ProductName|   Category|Price|SaleID|Quantity|\n",
            "+---------+-----------+-----------+-----+------+--------+\n",
            "|        1|     Laptop|Electronics|50000|     1|       2|\n",
            "|        1|     Laptop|Electronics|50000|     4|       1|\n",
            "|        2| Smartphone|Electronics|30000|     2|       1|\n",
            "|        2| Smartphone|Electronics|30000|     6|       2|\n",
            "|        3|      Table|  Furniture|15000|     3|       3|\n",
            "|        3|      Table|  Furniture|15000|     8|       1|\n",
            "|        4|      Chair|  Furniture| 5000|     5|       5|\n",
            "|        5| Headphones|Electronics| 2000|     7|      10|\n",
            "+---------+-----------+-----------+-----+------+--------+\n",
            "\n",
            "Total Sales DataFrame:\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "|ProductID|ProductName|   Category|Price|SaleID|Quantity|TotalSales|\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "|        1|     Laptop|Electronics|50000|     1|       2|    100000|\n",
            "|        1|     Laptop|Electronics|50000|     4|       1|     50000|\n",
            "|        2| Smartphone|Electronics|30000|     2|       1|     30000|\n",
            "|        2| Smartphone|Electronics|30000|     6|       2|     60000|\n",
            "|        3|      Table|  Furniture|15000|     3|       3|     45000|\n",
            "|        3|      Table|  Furniture|15000|     8|       1|     15000|\n",
            "|        4|      Chair|  Furniture| 5000|     5|       5|     25000|\n",
            "|        5| Headphones|Electronics| 2000|     7|      10|     20000|\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "\n",
            "Total Sales by Product Category:\n",
            "+-----------+------------------+\n",
            "|   Category|TotalSalesCategory|\n",
            "+-----------+------------------+\n",
            "|Electronics|            260000|\n",
            "|  Furniture|             85000|\n",
            "+-----------+------------------+\n",
            "\n",
            "Sorted Products by Total Sales Value:\n",
            "+-----------+-----------------+\n",
            "|ProductName|TotalSalesProduct|\n",
            "+-----------+-----------------+\n",
            "|     Laptop|           150000|\n",
            "| Smartphone|            90000|\n",
            "|      Table|            60000|\n",
            "|      Chair|            25000|\n",
            "| Headphones|            20000|\n",
            "+-----------+-----------------+\n",
            "\n",
            "Number of Sales for Each Product:\n",
            "+-----------+----------+\n",
            "|ProductName|SalesCount|\n",
            "+-----------+----------+\n",
            "|      Chair|         1|\n",
            "|     Laptop|         2|\n",
            "|      Table|         2|\n",
            "| Smartphone|         2|\n",
            "| Headphones|         1|\n",
            "+-----------+----------+\n",
            "\n",
            "Products with Total Sales Value Greater Than ₹50,000:\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "|ProductID|ProductName|   Category|Price|SaleID|Quantity|TotalSales|\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "|        1|     Laptop|Electronics|50000|     1|       2|    100000|\n",
            "|        2| Smartphone|Electronics|30000|     6|       2|     60000|\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "  .appName(\"RDD Transformation Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#Get the SparkContext from the SparkSession\n",
        "sc = spark.sparkContext\n",
        "print(\"Spark Session Created\")\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "\n",
        "# Print the original RDD\n",
        "print(\"Original RDD:\", rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M57lndsN8AP",
        "outputId": "b8d02562-cc9c-4f34-8c03-22ad303a8d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Created\n",
            "Original RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = rdd.map(lambda x: x* 2)\n",
        "\n",
        "# Print the transformed RDD\n",
        "\n",
        "print(\"RDD after map transformation (x2):\", rdd2.collect())\n",
        "\n",
        "\n",
        "rdd3 = rdd2.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Print the filtered RDD\n",
        "\n",
        "print(\"RDD after filter transformation (even numbers):\", rdd3.collect())\n",
        "\n",
        "\n",
        "\n",
        "sentences = [\"Hello world\", \"PySpark is great\" \"RDD transformations\"]\n",
        "\n",
        "rdd4 = sc.parallelize (sentences)\n",
        "\n",
        "words_rdd = rdd4.flatMap(lambda sentence: sentence.split(\" \"))\n",
        "\n",
        "#Print the flatMapped RDD\n",
        "\n",
        "print(\"RDD after flatMap transformation (split into words):\", words_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xg8N_LWOSjn",
        "outputId": "af0433ab-5e29-48f8-807d-d39079928df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD after map transformation (x2): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
            "RDD after filter transformation (even numbers): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
            "RDD after flatMap transformation (split into words): ['Hello', 'world', 'PySpark', 'is', 'greatRDD', 'transformations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = rdd3.collect()\n",
        "\n",
        "print(results)\n",
        "\n",
        "\n",
        "count = rdd3.count()\n",
        "\n",
        "print(f\"Number of elements: {count}\")\n",
        "\n",
        "\n",
        "total_sum =rdd.reduce(lambda x, y: x + y)\n",
        "print(\"Total sum: (total_sum)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjCcJqUeSkVe",
        "outputId": "e573bab3-4ae3-4622-8519-e598dfbee680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
            "Number of elements: 10\n",
            "Total sum: (total_sum)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating a DataFrame from a dictionary\n",
        "data = {'Name': ['John', 'Emma', 'Alex'],\n",
        "        'Age': [28, 32, 25],\n",
        "        'City': ['New York', 'London', 'Paris']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Creating a DataFrame from a list of lists\n",
        "data = [['John', 28, 'New York'],\n",
        "        ['Emma', 32, 'London'],\n",
        "        ['Alex', 25, 'Paris']]\n",
        "df = pd.DataFrame(data, columns=['Name', 'Age', 'City'])"
      ],
      "metadata": {
        "id": "TpLVehiEyx-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Get basic information about the DataFrame\n",
        "print(df.info())\n",
        "\n",
        "# Get statistical summary of numerical columns\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jFaFNrAy0Yj",
        "outputId": "46639526-6ecc-49e9-8c0c-c64c0e37287b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Name  Age      City\n",
            "0  John   28  New York\n",
            "1  Emma   32    London\n",
            "2  Alex   25     Paris\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3 entries, 0 to 2\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Name    3 non-null      object\n",
            " 1   Age     3 non-null      int64 \n",
            " 2   City    3 non-null      object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 200.0+ bytes\n",
            "None\n",
            "             Age\n",
            "count   3.000000\n",
            "mean   28.333333\n",
            "std     3.511885\n",
            "min    25.000000\n",
            "25%    26.500000\n",
            "50%    28.000000\n",
            "75%    30.000000\n",
            "max    32.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a single column\n",
        "ages = df['Age']\n",
        "# display the column\n",
        "print(ages)\n",
        "\n",
        "# Select multiple columns\n",
        "subset = df[['Name', 'City']]\n",
        "print(subset)\n",
        "# Select rows based on index\n",
        "first_two = df.loc[0:1]\n",
        "print(first_two)"
      ],
      "metadata": {
        "id": "EPvhLLqjy4jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a329118-a67b-41ba-e809-2556bd7d22eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    28\n",
            "1    32\n",
            "2    25\n",
            "Name: Age, dtype: int64\n",
            "   Name      City\n",
            "0  John  New York\n",
            "1  Emma    London\n",
            "2  Alex     Paris\n",
            "   Name  Age      City\n",
            "0  John   28  New York\n",
            "1  Emma   32    London\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns\n",
        "df = df.rename(columns={'Name': 'Full Name', 'City': 'Location'})\n",
        "print(df)"
      ],
      "metadata": {
        "id": "g5yYoq_Sy82B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows based on a condition\n",
        "young_people = df[df['Age'] < 30]\n",
        "print(young_people)\n",
        "# Multiple conditions\n",
        "young_new_yorkers = df[(df['Age'] < 30) & (df['City'] == 'New York')]\n",
        "print(young_new_yorkers)"
      ],
      "metadata": {
        "id": "MKBLbXgAzCBu",
        "outputId": "88568b65-b95a-4ff5-a193-8649c858ac85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Name  Age      City\n",
            "0  John   28  New York\n",
            "2  Alex   25     Paris\n",
            "   Name  Age      City\n",
            "0  John   28  New York\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sep 04"
      ],
      "metadata": {
        "id": "A5zFLHgktaPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Employee Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample employee data\n",
        "data = [\n",
        "    (1, 'Arjun', 'IT', 75000),\n",
        "    (2, 'Vijay', 'Finance', 85000),\n",
        "    (3, 'Shalini', 'IT', 90000),\n",
        "    (4, 'Sneha', 'HR', 50000),\n",
        "    (5, 'Rahul', 'Finance', 60000),\n",
        "    (6, 'Amit', 'IT', 55000)\n",
        "]\n",
        "\n",
        "# Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "# Create DataFrame\n",
        "employee_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "employee_df.show()\n",
        "\n",
        "# Filter employees who have a salary greater than 60,000\n",
        "high_salary_df = employee_df.filter(col('Salary') > 60000)\n",
        "\n",
        "# Calculate the average salary by department\n",
        "avg_salary_df = employee_df.groupBy(\"Department\").avg(\"Salary\")\n",
        "avg_salary_df.show()\n",
        "\n",
        "# Sort employees in descending order of salary\n",
        "sorted_df = employee_df.orderBy(col(\"Salary\").desc())\n",
        "sorted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRG_gq_-sPro",
        "outputId": "12ffa2a8-9f8c-4e5a-94f6-08a0074abde8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "|         5|       Rahul|   Finance| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+-----------------+\n",
            "|Department|      avg(Salary)|\n",
            "+----------+-----------------+\n",
            "|   Finance|          72500.0|\n",
            "|        IT|73333.33333333333|\n",
            "|        HR|          50000.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         5|       Rahul|   Finance| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "+----------+------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Employee Data Handling\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample employee data with null values\n",
        "data = [\n",
        "    (1, 'Arjun', 'IT', 75000),\n",
        "    (2, 'Vijay', 'Finance', 85000),\n",
        "    (3, None, 'IT', 90000),\n",
        "    (4, 'Sneha', 'HR', None),\n",
        "    (5, 'Rahul', None, 60000),\n",
        "    (6, 'Amit', 'IT', 55000)\n",
        "]\n",
        "\n",
        "# Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "# Create DataFrame\n",
        "employee_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "employee_df.show()\n",
        "\n",
        "# Fill null values in 'EmployeeName' and 'Department' with 'Unknown'\n",
        "filled_df = employee_df.fillna({'EmployeeName': 'Unknown', 'Department': 'Unknown'})\n",
        "filled_df.show()\n",
        "\n",
        "# Drop rows where 'Salary' is null\n",
        "dropped_null_salary_df = employee_df.dropna(subset=['Salary'])\n",
        "dropped_null_salary_df.show()\n",
        "\n",
        "# Fill null values in 'Salary' with 50000\n",
        "salary_filled_df = employee_df.fillna({'Salary': 50000})\n",
        "salary_filled_df.show()\n",
        "\n",
        "# Check for null values in the entire DataFrame\n",
        "null_counts = employee_df.select([col(c).isNull().alias(c) for c in employee_df.columns]).show()\n",
        "\n",
        "# Replace all null values in the DataFrame with 'N/A'\n",
        "na_filled_df = employee_df.na.fill('N/A')\n",
        "na_filled_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C1cCk9j3Vlp",
        "outputId": "5a748a15-8be3-4bbd-b6a3-280a68df69a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Unknown|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|   Unknown| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|     false|       false|     false| false|\n",
            "|     false|       false|     false| false|\n",
            "|     false|        true|     false| false|\n",
            "|     false|       false|     false|  true|\n",
            "|     false|       false|      true| false|\n",
            "|     false|       false|     false| false|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|         N/A|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|       N/A| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, col, sum\n",
        "\n",
        "# Initialize a Spark Session\n",
        "spark = SparkSession.builder\\\n",
        "    .appName(\"Advanced Dataframe Operations\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data1 = [\n",
        "    (1, 'Arjun', 'IT', 75000, '2022-01-15'),\n",
        "    (2, 'Vijay', 'Finance', 85000, '2022-03-12'),\n",
        "    (3, 'Shalini', 'IT', 90000, '2021-06-30')\n",
        "]\n",
        "\n",
        "data2 = [\n",
        "    (4, 'Sneha', 'HR', 50000, '2022-05-01'),\n",
        "    (5, 'Rahul', 'Finance', 60000, '2022-08-20'),\n",
        "    (6, 'Amit', 'IT', 55000, '2021-12-15')\n",
        "]\n",
        "\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary', 'JoiningDate']\n",
        "\n",
        "# Create DataFrames\n",
        "employee_df1 = spark.createDataFrame(data=data1, schema=columns)\n",
        "employee_df2 = spark.createDataFrame(data=data2, schema=columns)\n",
        "\n",
        "employee_df1.show()\n",
        "employee_df2.show()\n",
        "\n",
        "# Union two DataFrames (removes duplicates)\n",
        "union_df = employee_df1.union(employee_df2).dropDuplicates()\n",
        "print(\"Union of two dataframes (Remove duplicates): \")\n",
        "union_df.show()\n",
        "\n",
        "# Union of two DataFrames (includes everything)\n",
        "union_all_df = employee_df1.union(employee_df2)\n",
        "print(\"Union of two dataframes: \")\n",
        "union_all_df.show()\n",
        "\n",
        "# Rank employees by salary within each department\n",
        "window_spec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
        "ranked_df = union_all_df.withColumn(\"Rank\", rank().over(window_spec))\n",
        "ranked_df.show()\n",
        "\n",
        "# Convert 'JoiningDate' from string to date type\n",
        "date_converted_df = union_all_df.withColumn(\"JoiningDate\", F.to_date(col(\"JoiningDate\"), \"yyyy-MM-dd\"))\n",
        "date_converted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2CAOF0m6wiI",
        "outputId": "6772706e-a798-4ed4-cf76-1ec0426904f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "Union of two dataframes (Remove duplicates): \n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "Union of two dataframes: \n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|Rank|\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|   1|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|   2|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|   1|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|   1|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|   2|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|   3|\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "# Define a window specification for cumulative sum of salaries within each department\n",
        "window_spec_sum = Window.partitionBy(\"Department\").orderBy(\"JoiningDate\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Calculate the running total of salaries\n",
        "running_total_df = date_converted_df.withColumn(\"RunningTotal\", sum(col(\"Salary\")).over(window_spec_sum))\n",
        "running_total_df.show()\n",
        "\n",
        "# Calculate the number of years since joining\n",
        "experience_df = running_total_df.withColumn(\"YearsOfExperience\", F.round(F.datediff(F.current_date(), col(\"JoiningDate\")) / 365, 2))\n",
        "experience_df.show()\n",
        "\n",
        "# Add a new column for next evaluation date (one year after joining)\n",
        "eval_date_df = experience_df.withColumn(\"NextEvaluationDate\", F.date_add(col(\"JoiningDate\"), 365))\n",
        "eval_date_df.show()\n",
        "\n",
        "# Calculate average salary per department\n",
        "avg_salary_df = union_all_df.groupBy(\"Department\").agg(F.avg(\"Salary\").alias(\"AverageSalary\"))\n",
        "avg_salary_df.show()\n",
        "\n",
        "# Calculate the total number of employees\n",
        "total_employees_df = union_all_df.agg(F.count(\"EmployeeID\").alias(\"TotalEmployees\"))\n",
        "total_employees_df.show()\n",
        "\n",
        "# Convert employee names to uppercase\n",
        "uppercase_name_df = union_all_df.withColumn(\"EmployeeNameUpper\", F.upper(col(\"EmployeeName\")))\n",
        "uppercase_name_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcmy0OSFS3ev",
        "outputId": "5b84ca33-37ec-4341-a7b0-dbf0f0cb0843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+-----------+------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|RunningTotal|\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|       85000|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|      145000|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|       50000|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|       90000|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|      145000|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|      220000|\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|RunningTotal|YearsOfExperience|\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|       85000|             2.48|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|      145000|             2.04|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|       50000|             2.35|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|       90000|             3.18|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|      145000|             2.72|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|      220000|             2.64|\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+------------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|RunningTotal|YearsOfExperience|NextEvaluationDate|\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+------------------+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|       85000|             2.48|        2023-03-12|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|      145000|             2.04|        2023-08-20|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|       50000|             2.35|        2023-05-01|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|       90000|             3.18|        2022-06-30|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|      145000|             2.72|        2022-12-15|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|      220000|             2.64|        2023-01-15|\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+------------------+\n",
            "\n",
            "+----------+-----------------+\n",
            "|Department|    AverageSalary|\n",
            "+----------+-----------------+\n",
            "|        IT|73333.33333333333|\n",
            "|   Finance|          72500.0|\n",
            "|        HR|          50000.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "+--------------+\n",
            "|TotalEmployees|\n",
            "+--------------+\n",
            "|             6|\n",
            "+--------------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|EmployeeNameUpper|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|            ARJUN|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|            VIJAY|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|          SHALINI|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|            SNEHA|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|            RAHUL|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|             AMIT|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sep 05"
      ],
      "metadata": {
        "id": "jTF6NrpPaAZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataIngesttion\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "csv_file_path = \"/content/people.csv\"\n",
        "df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file_path)\n",
        "df_csv.show()"
      ],
      "metadata": {
        "id": "wP3E8WLVTYO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53c8f15-0811-44a7-9933-2ddfffe58826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-------+\n",
            "|Name| Age| Gender|\n",
            "+----+----+-------+\n",
            "|John|  28|   Male|\n",
            "|Jane|  32| Female|\n",
            "+----+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define the schema for the JSON file (Note only after defining the schema it works fine on colab)\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"address\", StructType([\n",
        "        StructField(\"street\", StringType(), True),\n",
        "        StructField(\"city\", StringType(), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "\n",
        "# Load the complex JSON file with the correct path\n",
        "json_file_path = \"/content/Sample.json\"\n",
        "\n",
        "# Read the JSON file with schema\n",
        "df_json_complex = spark.read.schema(schema).json(json_file_path)\n",
        "\n",
        "# Read the file as text to inspect its contents\n",
        "with open(json_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "    print(data)"
      ],
      "metadata": {
        "id": "rjv8-V0IX-t9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85fb3905-d447-4097-f592-c50d6ef86478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"name\": \"John\",\n",
            "    \"age\": 28,\n",
            "    \"gender\": \"Male\",\n",
            "    \"address\": {\n",
            "      \"street\": \"123 Main St\",\n",
            "      \"city\": \"New York\"\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"name\": \"Jane\",\n",
            "    \"age\": 32,\n",
            "    \"gender\": \"Female\",\n",
            "    \"address\": {\n",
            "      \"street\": \"456 Elm St\",\n",
            "      \"city\": \"San Francisco\"\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = {\n",
        "    \"name\": [\"John\", \"Jane\", \"Mike\", \"Emily\"],\n",
        "    \"age\": [28, 32, 45, 23],\n",
        "    \"gender\": [\"Male\", \"Female\", \"Male\", \"Female\"],\n",
        "    \"city\": [\"New York\", \"San Francisco\", \"Los Angeles\", \"Chicago\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file in the Colab environment\n",
        "csv_file_path = \"/content/sample_people.csv\"\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Confirm the file has been created\n",
        "print(f\"CSV file created at: {csv_file_path}\")\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"CreateViewExample\").getOrCreate()\n",
        "\n",
        "# Load the CSV file into a PySpark DataFrame\n",
        "df_people = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(csv_file_path)\n",
        "\n",
        "# Show the DataFrame\n",
        "df_people.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCYiBWksf4Nt",
        "outputId": "f975eacf-8472-4ce1-9028-65e16defcd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created at: /content/sample_people.csv\n",
            "+-----+---+------+-------------+\n",
            "| name|age|gender|         city|\n",
            "+-----+---+------+-------------+\n",
            "| John| 28|  Male|     New York|\n",
            "| Jane| 32|Female|San Francisco|\n",
            "| Mike| 45|  Male|  Los Angeles|\n",
            "|Emily| 23|Female|      Chicago|\n",
            "+-----+---+------+-------------+\n",
            "\n",
            "+----+---+------+-------------+\n",
            "|name|age|gender|         city|\n",
            "+----+---+------+-------------+\n",
            "|Jane| 32|Female|San Francisco|\n",
            "|Mike| 45|  Male|  Los Angeles|\n",
            "+----+---+------+-------------+\n",
            "\n",
            "+-----+---+--------+\n",
            "| name|age|    city|\n",
            "+-----+---+--------+\n",
            "| John| 28|New York|\n",
            "|Emily| 23| Chicago|\n",
            "+-----+---+--------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary view\n",
        "df_people.createOrReplaceTempView(\"people_temp_view\")\n",
        "\n",
        "# Run an SQL query on the view\n",
        "result_temp_view = spark.sql(\"SELECT name, age, gender, city FROM people_temp_view WHERE age > 30\")\n",
        "\n",
        "# Show the result\n",
        "result_temp_view.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbkUoWS3lEUi",
        "outputId": "b18fb437-3847-439c-e0a3-e042edb28dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+------+-------------+\n",
            "|name|age|gender|         city|\n",
            "+----+---+------+-------------+\n",
            "|Jane| 32|Female|San Francisco|\n",
            "|Mike| 45|  Male|  Los Angeles|\n",
            "+----+---+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a global temporary view\n",
        "df_people.createOrReplaceGlobalTempView(\"people_global_view\")\n",
        "\n",
        "# Query the global temporary view\n",
        "result_global_view = spark.sql(\"SELECT name, age, city FROM global_temp.people_global_view WHERE age < 30\")\n",
        "\n",
        "# Show the result\n",
        "result_global_view.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMhlglk5lIAX",
        "outputId": "4c171085-74cc-45ac-be9c-f36a7ba26044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+--------+\n",
            "| name|age|    city|\n",
            "+-----+---+--------+\n",
            "| John| 28|New York|\n",
            "|Emily| 23| Chicago|\n",
            "+-----+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List all temporary views and tables\n",
        "spark.catalog.listTables()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOH6wzsLlKWV",
        "outputId": "ff14b9b8-7ca8-4bf7-89cd-4b8b15be3e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='people_temp_view', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the local temporary view\n",
        "spark.catalog.dropTempView(\"people_temp_view\")\n",
        "\n",
        "# Drop the global temporary view\n",
        "spark.catalog.dropGlobalTempView(\"people_global_view\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuwq2FZMlL5-",
        "outputId": "d62b9c12-f633-49c4-e4c1-70885cd3b226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afternoon"
      ],
      "metadata": {
        "id": "aZT9nuhxeDo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full refresh: Load the entire dataset\n",
        "df_sales = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"/content/sales_data.csv\")\n",
        "\n",
        "# Apply transformation (if necessary)\n",
        "df_transformed = df_sales.withColumn(\"total_sales\", df_sales[\"quantity\"] * df_sales[\"price\"])\n",
        "\n",
        "# Full refresh: Partition the data by 'date' and overwrite the existing data\n",
        "output_path = \"/content/partitioned_data\"\n",
        "df_transformed.write.partitionBy(\"date\").mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "# Verify partitioned data\n",
        "partitioned_df = spark.read.parquet(output_path)\n",
        "partitioned_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ajv9BHFeDUE",
        "outputId": "305feef0-e052-4837-c331-e8e2dda0951f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|transaction_id|customer_id| product|quantity|price|         updated_at|total_sales|      date|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|             1|        101|  Laptop|       1| 1000|2024-09-01 08:00:00|       1000|2024-09-01|\n",
            "|             2|        102|   Phone|       2|  500|2024-09-01 09:00:00|       1000|2024-09-01|\n",
            "|             5|        105|Keyboard|       1|   50|2024-09-03 12:00:00|         50|2024-09-03|\n",
            "|             6|        106|   Mouse|       3|   30|2024-09-03 13:00:00|         90|2024-09-03|\n",
            "|             3|        103|  Tablet|       1|  300|2024-09-02 10:00:00|        300|2024-09-02|\n",
            "|             4|        104| Monitor|       2|  200|2024-09-02 11:00:00|        400|2024-09-02|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Incremental load: Define the last ETL run timestamp (this should be tracked externally)\n",
        "last_etl_run = \"2024-01-01 00:00:00\"\n",
        "\n",
        "# Load only new or updated records since the last ETL run\n",
        "df_incremental = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"/content/sales_data.csv\") \\\n",
        "    .filter(F.col(\"updated_at\") > last_etl_run)\n",
        "\n",
        "# Apply transformations (if necessary)\n",
        "df_transformed_incremental = df_incremental.withColumn(\"total_sales\", df_incremental[\"quantity\"] * df_incremental[\"price\"])\n",
        "\n",
        "# Incremental load: Append the new data to the existing partitioned dataset\n",
        "output_path = \"/content/partitioned_sales_data\"\n",
        "df_transformed_incremental.write.partitionBy(\"date\").mode(\"append\").parquet(output_path)\n",
        "\n",
        "# Verify partitioned data after incremental load\n",
        "partitioned_df = spark.read.parquet(output_path)\n",
        "partitioned_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJq0jWcXnjwE",
        "outputId": "e3b0cbe6-0688-4792-dba5-9a2f40f8bab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|transaction_id|customer_id| product|quantity|price|         updated_at|total_sales|      date|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|             1|        101|  Laptop|       1| 1000|2024-09-01 08:00:00|       1000|2024-09-01|\n",
            "|             2|        102|   Phone|       2|  500|2024-09-01 09:00:00|       1000|2024-09-01|\n",
            "|             5|        105|Keyboard|       1|   50|2024-09-03 12:00:00|         50|2024-09-03|\n",
            "|             6|        106|   Mouse|       3|   30|2024-09-03 13:00:00|         90|2024-09-03|\n",
            "|             3|        103|  Tablet|       1|  300|2024-09-02 10:00:00|        300|2024-09-02|\n",
            "|             4|        104| Monitor|       2|  200|2024-09-02 11:00:00|        400|2024-09-02|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ipywidgets in Colab or Jupyter if needed\n",
        "!pip install ipywidgets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtD4mrO5tYQV",
        "outputId": "0765965c-dd63-4d23-b2ff-8fae37072dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.8)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Step 1: Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"PySpark with Widgets Example\").getOrCreate()\n",
        "\n",
        "# Step 2: Create a simple DataFrame\n",
        "data = [\n",
        "    (\"John\", 28, \"Male\", 60000),\n",
        "    (\"Jane\", 32, \"Female\", 72000),\n",
        "    (\"Mike\", 45, \"Male\", 84000),\n",
        "    (\"Emily\", 23, \"Female\", 52000),\n",
        "    (\"Alex\", 36, \"Male\", 67000)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"gender\", \"salary\"])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJe_LhIatcmy",
        "outputId": "467e305f-84d6-4c02-d689-f6fc39616cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+------+\n",
            "| name|age|gender|salary|\n",
            "+-----+---+------+------+\n",
            "| John| 28|  Male| 60000|\n",
            "| Jane| 32|Female| 72000|\n",
            "| Mike| 45|  Male| 84000|\n",
            "|Emily| 23|Female| 52000|\n",
            "| Alex| 36|  Male| 67000|\n",
            "+-----+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create widgets\n",
        "\n",
        "# Dropdown widget to select column for filtering\n",
        "column_dropdown = widgets.Dropdown(\n",
        "    options=[\"age\", \"salary\"],\n",
        "    value=\"age\",\n",
        "    description=\"Filter By:\",\n",
        ")\n",
        "\n",
        "# Slider widget to choose a value for filtering\n",
        "slider = widgets.IntSlider(\n",
        "    value=30,\n",
        "    min=0,\n",
        "    max=100,\n",
        "    step=5,\n",
        "    description=\"Threshold:\",\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "# Button to trigger filtering\n",
        "button = widgets.Button(description=\"Apply Filter\")\n",
        "\n",
        "# Output area to show the results\n",
        "output = widgets.Output()\n",
        "\n",
        "# Display the widgets\n",
        "display(column_dropdown, slider, button, output)\n",
        "\n",
        "# Step 4: Define the function to apply filtering based on widget inputs\n",
        "def apply_filter(b):\n",
        "    column = column_dropdown.value\n",
        "    threshold = slider.value\n",
        "\n",
        "    # Clear previous output\n",
        "    output.clear_output()\n",
        "\n",
        "    # Filter the DataFrame based on widget values\n",
        "    df_filtered = df.filter(df[column] > threshold)\n",
        "\n",
        "    # Show the filtered DataFrame\n",
        "    with output:\n",
        "        print(f\"Filtering by {column} > {threshold}\")\n",
        "        df_filtered.show()\n",
        "\n",
        "# Step 5: Attach the function to the button click event\n",
        "button.on_click(apply_filter)"
      ],
      "metadata": {
        "id": "3_lZW70ktiVM",
        "outputId": "762d3dd4-c59a-4aec-f212-4422f773cb07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111,
          "referenced_widgets": [
            "c016a4a779c04721be55ca865ab1b876",
            "9333018ea6a84aa19f848985300429e3",
            "d09d061d7d244e01a7905e57802b977c",
            "8b7c4f3c94a64ed4b212552318cfa3c2",
            "ee96ea39b0174de083241263abed3ead",
            "9a869f8dd3704664a7703728f8ef6369",
            "667e7ca523d643118f3fac71f5003de4",
            "69acc13ffc6e4b3481b1ab38f8b457ea",
            "6e48ea452abd445c8657b7c1092a2256",
            "0f4475c8582140898fb757aa06e1ed2b",
            "9355276f219c482393407d6e60c210f1"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Filter By:', options=('age', 'salary'), value='age')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c016a4a779c04721be55ca865ab1b876"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntSlider(value=30, continuous_update=False, description='Threshold:', step=5)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b7c4f3c94a64ed4b212552318cfa3c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Apply Filter', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "667e7ca523d643118f3fac71f5003de4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f4475c8582140898fb757aa06e1ed2b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_csv(filename, num_records=1000000):\n",
        "    event_types = [\"purchase\", \"view\", \"click\"]\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"event_time\", \"event_type\", \"user_id\", \"amount\"])  # Header\n",
        "\n",
        "        for i in range(num_records):\n",
        "            event_time = start_time + timedelta(seconds=i)\n",
        "            event_type = random.choice(event_types)\n",
        "            user_id = f\"user_{random.randint(1, 10000)}\"\n",
        "            amount = round(random.uniform(1, 100), 2)\n",
        "\n",
        "            writer.writerow([event_time, event_type, user_id, amount])\n",
        "\n",
        "            if i % 100000 == 0:\n",
        "                print(f\"Generated {i} records...\")\n",
        "\n",
        "    print(f\"CSV file '{filename}' with {num_records} records has been generated.\")\n",
        "\n",
        "# Generate the CSV file\n",
        "generate_csv(\"million_records.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsdakvVKW0e7",
        "outputId": "2388df99-5887-4d05-c747-dbd4bc6b2fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 0 records...\n",
            "Generated 100000 records...\n",
            "Generated 200000 records...\n",
            "Generated 300000 records...\n",
            "Generated 400000 records...\n",
            "Generated 500000 records...\n",
            "Generated 600000 records...\n",
            "Generated 700000 records...\n",
            "Generated 800000 records...\n",
            "Generated 900000 records...\n",
            "CSV file 'million_records.csv' with 1000000 records has been generated.\n"
          ]
        }
      ]
    }
  ]
}