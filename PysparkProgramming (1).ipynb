{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndsX_NnBjkrz",
        "outputId": "1242e7bf-d9a4-4447-98d0-4b785c96e7ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"PySpark DataFrame Example\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "# Sample data representing employees\n",
        "\n",
        "data = [\n",
        "\n",
        "    (\"John Doe\", \"Engineering\", 75000),\n",
        "\n",
        "    (\"Jane Smith\", \"Marketing\", 60000),\n",
        "\n",
        "    (\"Sam Brown\", \"Engineering\", 80000),\n",
        "\n",
        "    (\"Emily Davis\", \"HR\", 50000),\n",
        "\n",
        "    (\"Michael Johnson\", \"Marketing\", 70000),\n",
        "\n",
        "]\n",
        "\n",
        "# Define schema for DataFrame\n",
        "\n",
        "columns = [\"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "# Create DataFrame\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "\n",
        "df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7WZRXankHOe",
        "outputId": "4415f61d-109e-458f-8775-59e3dcd0605f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+------+\n",
            "|           Name| Department|Salary|\n",
            "+---------------+-----------+------+\n",
            "|       John Doe|Engineering| 75000|\n",
            "|     Jane Smith|  Marketing| 60000|\n",
            "|      Sam Brown|Engineering| 80000|\n",
            "|    Emily Davis|         HR| 50000|\n",
            "|Michael Johnson|  Marketing| 70000|\n",
            "+---------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter: Select employees with a salary greater than 65,000\n",
        "high_salary_df = df.filter(col(\"Salary\") > 65000)\n",
        "print(\"Employees with Salary > 65,000:\")\n",
        "# Show the filtered DataFrame\n",
        "high_salary_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehpWz3__qRR2",
        "outputId": "6fcced8c-b856-484e-8b76-61616eaa1d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees with Salary > 65,000:\n",
            "+---------------+-----------+------+\n",
            "|           Name| Department|Salary|\n",
            "+---------------+-----------+------+\n",
            "|       John Doe|Engineering| 75000|\n",
            "|      Sam Brown|Engineering| 80000|\n",
            "|Michael Johnson|  Marketing| 70000|\n",
            "+---------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Department and Calculate the average salary\n",
        "avg_salary_by_df = df.groupBy(\"Department\").avg(\"Salary\")\n",
        "print(\"Average Salary by Department:\")\n",
        "# Show the average salary by department\n",
        "avg_salary_by_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfJ6ZVpArCuR",
        "outputId": "4d644f7f-f611-4f14-d523-9dfd7a9d0193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Salary by Department:\n",
            "+-----------+-----------+\n",
            "| Department|avg(Salary)|\n",
            "+-----------+-----------+\n",
            "|Engineering|    77500.0|\n",
            "|  Marketing|    65000.0|\n",
            "|         HR|    50000.0|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Customer Transaction Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data for customers\n",
        "customers = [\n",
        "    (1, \"Ravi\", \"Mumbai\"),\n",
        "    (2, \"Priya\", \"Delhi\"),\n",
        "    (3, \"Vijay\", \"Bangalore\"),\n",
        "    (4, \"Anita\", \"Chennai\"),\n",
        "    (5, \"Raj\", \"Hyderabad\"),\n",
        "]\n",
        "\n",
        "# Sample data for transactions\n",
        "transactions = [\n",
        "    (1, 1, 10000.50),\n",
        "    (2, 2, 20000.75),\n",
        "    (3, 1, 15000.25),\n",
        "    (4, 3, 30000.00),\n",
        "    (5, 2, 40000.50),\n",
        "    (6, 4, 25000.00),\n",
        "    (7, 5, 18000.75),\n",
        "    (8, 1, 5000.00),\n",
        "]\n",
        "\n",
        "# Define schema for DataFrames\n",
        "customer_columns = [\"customer_id\", \"Name\", \"city\"]\n",
        "transaction_columns = [\"Transaction_id\", \"customer_id\", \"Amount\"]  # Changed CustomerId to customer_id\n",
        "\n",
        "# Create DataFrames\n",
        "customer_df = spark.createDataFrame(customers, schema=customer_columns)\n",
        "transactions_df = spark.createDataFrame(transactions, schema=transaction_columns)\n",
        "\n",
        "# Show the DataFrames\n",
        "print(\"Customers DataFrame:\")\n",
        "customer_df.show()\n",
        "\n",
        "print(\"Transactions DataFrame:\")\n",
        "transactions_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DKP07IxBMfG",
        "outputId": "e5a27045-3226-43f8-9cb1-f01302d9cdf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers DataFrame:\n",
            "+-----------+-----+---------+\n",
            "|customer_id| Name|     city|\n",
            "+-----------+-----+---------+\n",
            "|          1| Ravi|   Mumbai|\n",
            "|          2|Priya|    Delhi|\n",
            "|          3|Vijay|Bangalore|\n",
            "|          4|Anita|  Chennai|\n",
            "|          5|  Raj|Hyderabad|\n",
            "+-----------+-----+---------+\n",
            "\n",
            "Transactions DataFrame:\n",
            "+--------------+-----------+--------+\n",
            "|Transaction_id|customer_id|  Amount|\n",
            "+--------------+-----------+--------+\n",
            "|             1|          1| 10000.5|\n",
            "|             2|          2|20000.75|\n",
            "|             3|          1|15000.25|\n",
            "|             4|          3| 30000.0|\n",
            "|             5|          2| 40000.5|\n",
            "|             6|          4| 25000.0|\n",
            "|             7|          5|18000.75|\n",
            "|             8|          1|  5000.0|\n",
            "+--------------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the DataFrames on customer_id\n",
        "customer_transactions_df = customer_df.join(transactions_df, on=\"customer_id\")\n",
        "print(\"Customer Transactions DataFrame:\")\n",
        "customer_transactions_df.show()\n",
        "\n",
        "# Calculate the total amount spent by each customer\n",
        "total_spent_df = customer_transactions_df.groupBy(\"Name\").sum(\"Amount\").withColumnRenamed(\"sum(Amount)\", \"TotalSpent\")\n",
        "print(\"Total Amount Spent by Each Customer:\")\n",
        "total_spent_df.show()\n",
        "\n",
        "# Find customers who have spent more than 30000\n",
        "big_spender_df = total_spent_df.filter(col(\"TotalSpent\") > 30000)\n",
        "print(\"Customers who have spent more than 30000:\")\n",
        "big_spender_df.show()\n",
        "\n",
        "# Count the number of transactions per customer\n",
        "transaction_count_df = customer_transactions_df.groupBy(\"Name\").count().withColumnRenamed(\"count\", \"TransactionCount\")\n",
        "print(\"Number of Transactions per Customer:\")\n",
        "transaction_count_df.show()\n",
        "\n",
        "# Sort customers by total amount spent in descending order\n",
        "sorted_spenders_df = total_spent_df.orderBy(col(\"TotalSpent\").desc())\n",
        "print(\"Customers Sorted by Total Amount Spent:\")\n",
        "sorted_spenders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeTrqbPHJCyy",
        "outputId": "9804dcc5-f81d-4337-e04d-2587f50f99a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Transactions DataFrame:\n",
            "+-----------+-----+---------+--------------+--------+\n",
            "|customer_id| Name|     city|Transaction_id|  Amount|\n",
            "+-----------+-----+---------+--------------+--------+\n",
            "|          1| Ravi|   Mumbai|             1| 10000.5|\n",
            "|          1| Ravi|   Mumbai|             3|15000.25|\n",
            "|          1| Ravi|   Mumbai|             8|  5000.0|\n",
            "|          2|Priya|    Delhi|             2|20000.75|\n",
            "|          2|Priya|    Delhi|             5| 40000.5|\n",
            "|          3|Vijay|Bangalore|             4| 30000.0|\n",
            "|          4|Anita|  Chennai|             6| 25000.0|\n",
            "|          5|  Raj|Hyderabad|             7|18000.75|\n",
            "+-----------+-----+---------+--------------+--------+\n",
            "\n",
            "Total Amount Spent by Each Customer:\n",
            "+-----+----------+\n",
            "| Name|TotalSpent|\n",
            "+-----+----------+\n",
            "| Ravi|  30000.75|\n",
            "|Priya|  60001.25|\n",
            "|Vijay|   30000.0|\n",
            "|Anita|   25000.0|\n",
            "|  Raj|  18000.75|\n",
            "+-----+----------+\n",
            "\n",
            "Customers who have spent more than 30000:\n",
            "+-----+----------+\n",
            "| Name|TotalSpent|\n",
            "+-----+----------+\n",
            "| Ravi|  30000.75|\n",
            "|Priya|  60001.25|\n",
            "+-----+----------+\n",
            "\n",
            "Number of Transactions per Customer:\n",
            "+-----+----------------+\n",
            "| Name|TransactionCount|\n",
            "+-----+----------------+\n",
            "| Ravi|               3|\n",
            "|Priya|               2|\n",
            "|Vijay|               1|\n",
            "|Anita|               1|\n",
            "|  Raj|               1|\n",
            "+-----+----------------+\n",
            "\n",
            "Customers Sorted by Total Amount Spent:\n",
            "+-----+----------+\n",
            "| Name|TotalSpent|\n",
            "+-----+----------+\n",
            "|Priya|  60001.25|\n",
            "| Ravi|  30000.75|\n",
            "|Vijay|   30000.0|\n",
            "|Anita|   25000.0|\n",
            "|  Raj|  18000.75|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjkmvufhZOGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### **Exercise: Product Sales Analysis**\n",
        "\n",
        "# #### **Step 1: Create DataFrames**\n",
        "\n",
        "# You will create two DataFrames: one for products and another for sales transactions. Then, you’ll perform operations like joining these DataFrames and analyzing the data.\n",
        "\n",
        "# ```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Product Sales Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data for products\n",
        "products = [\n",
        "    (1, \"Laptop\", \"Electronics\", 50000),\n",
        "    (2, \"Smartphone\", \"Electronics\", 30000),\n",
        "    (3, \"Table\", \"Furniture\", 15000),\n",
        "    (4, \"Chair\", \"Furniture\", 5000),\n",
        "    (5, \"Headphones\", \"Electronics\", 2000),\n",
        "]\n",
        "\n",
        "# Sample data for sales transactions\n",
        "sales = [\n",
        "    (1, 1, 2),\n",
        "    (2, 2, 1),\n",
        "    (3, 3, 3),\n",
        "    (4, 1, 1),\n",
        "    (5, 4, 5),\n",
        "    (6, 2, 2),\n",
        "    (7, 5, 10),\n",
        "    (8, 3, 1),\n",
        "]\n",
        "\n",
        "# Define schema for DataFrames\n",
        "product_columns = [\"ProductID\", \"ProductName\", \"Category\", \"Price\"]\n",
        "sales_columns = [\"SaleID\", \"ProductID\", \"Quantity\"]\n",
        "\n",
        "# Create DataFrames\n",
        "product_df = spark.createDataFrame(products, schema=product_columns)\n",
        "sales_df = spark.createDataFrame(sales, schema=sales_columns)\n",
        "\n",
        "# Show the DataFrames\n",
        "print(\"Products DataFrame:\")\n",
        "product_df.show()\n",
        "\n",
        "print(\"Sales DataFrame:\")\n",
        "sales_df.show()\n",
        "# ```\n",
        "\n",
        "# #### **Step 2: Perform the Following Tasks**\n",
        "\n",
        "# 1. **Join the DataFrames:**\n",
        "#    - Join the `product_df` and `sales_df` DataFrames on `ProductID` to create a combined DataFrame with product and sales data.\n",
        "product_and_sales_df = product_df.join(sales_df, on=\"ProductID\")\n",
        "print(\"Product and Sales DataFrame:\")\n",
        "product_and_sales_df.show()\n",
        "\n",
        "# 2. **Calculate Total Sales Value:**\n",
        "#    - For each product, calculate the total sales value by multiplying the price by the quantity sold.\n",
        "total_sales_df = product_and_sales_df.withColumn(\"TotalSales\", col(\"Price\") * col(\"Quantity\"))\n",
        "print(\"Total Sales DataFrame:\")\n",
        "total_sales_df.show()\n",
        "\n",
        "# 3. **Find the Total Sales for Each Product Category:**\n",
        "#    - Group the data by the `Category` column and calculate the total sales value for each product category.\n",
        "total_sales_by_category_df = total_sales_df.groupBy(\"Category\").sum(\"TotalSales\").withColumnRenamed(\"sum(TotalSales)\", \"TotalSalesCategory\")\n",
        "print(\"Total Sales by Product Category:\")\n",
        "total_sales_by_category_df.show()\n",
        "\n",
        "# 4. **Identify the Top-Selling Product:**\n",
        "#    - Find the product that generated the highest total sales value.\n",
        "top_selling_product_df = total_sales_df.groupBy(\"ProductName\").sum(\"TotalSales\").withColumnRenamed(\"sum(TotalSales)\", \"TotalSalesProduct\")\n",
        "top_selling_product_df\n",
        "\n",
        "# 5. **Sort the Products by Total Sales Value:**\n",
        "#    - Sort the products by total sales value in descending order.\n",
        "sorted_products_df = top_selling_product_df.orderBy(col(\"TotalSalesProduct\").desc())\n",
        "print(\"Sorted Products by Total Sales Value:\")\n",
        "sorted_products_df.show()\n",
        "\n",
        "# 6. **Count the Number of Sales for Each Product:**\n",
        "#    - Count the number of sales transactions for each product.\n",
        "sales_count_df = total_sales_df.groupBy(\"ProductName\").count().withColumnRenamed(\"count\", \"SalesCount\")\n",
        "print(\"Number of Sales for Each Product:\")\n",
        "sales_count_df.show()\n",
        "\n",
        "# 7. **Filter the Products with Total Sales Value Greater Than ₹50,000:**\n",
        "#    - Filter out the products that have a total sales value greater than ₹50,000.\n",
        "high_value_products_df = total_sales_df.filter(col(\"TotalSales\") > 50000)\n",
        "print(\"Products with Total Sales Value Greater Than ₹50,000:\")\n",
        "high_value_products_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2VRd67CRdMu",
        "outputId": "f5409ca8-720c-4ef2-fb2e-851defa196c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products DataFrame:\n",
            "+---------+-----------+-----------+-----+\n",
            "|ProductID|ProductName|   Category|Price|\n",
            "+---------+-----------+-----------+-----+\n",
            "|        1|     Laptop|Electronics|50000|\n",
            "|        2| Smartphone|Electronics|30000|\n",
            "|        3|      Table|  Furniture|15000|\n",
            "|        4|      Chair|  Furniture| 5000|\n",
            "|        5| Headphones|Electronics| 2000|\n",
            "+---------+-----------+-----------+-----+\n",
            "\n",
            "Sales DataFrame:\n",
            "+------+---------+--------+\n",
            "|SaleID|ProductID|Quantity|\n",
            "+------+---------+--------+\n",
            "|     1|        1|       2|\n",
            "|     2|        2|       1|\n",
            "|     3|        3|       3|\n",
            "|     4|        1|       1|\n",
            "|     5|        4|       5|\n",
            "|     6|        2|       2|\n",
            "|     7|        5|      10|\n",
            "|     8|        3|       1|\n",
            "+------+---------+--------+\n",
            "\n",
            "Product and Sales DataFrame:\n",
            "+---------+-----------+-----------+-----+------+--------+\n",
            "|ProductID|ProductName|   Category|Price|SaleID|Quantity|\n",
            "+---------+-----------+-----------+-----+------+--------+\n",
            "|        1|     Laptop|Electronics|50000|     1|       2|\n",
            "|        1|     Laptop|Electronics|50000|     4|       1|\n",
            "|        2| Smartphone|Electronics|30000|     2|       1|\n",
            "|        2| Smartphone|Electronics|30000|     6|       2|\n",
            "|        3|      Table|  Furniture|15000|     3|       3|\n",
            "|        3|      Table|  Furniture|15000|     8|       1|\n",
            "|        4|      Chair|  Furniture| 5000|     5|       5|\n",
            "|        5| Headphones|Electronics| 2000|     7|      10|\n",
            "+---------+-----------+-----------+-----+------+--------+\n",
            "\n",
            "Total Sales DataFrame:\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "|ProductID|ProductName|   Category|Price|SaleID|Quantity|TotalSales|\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "|        1|     Laptop|Electronics|50000|     1|       2|    100000|\n",
            "|        1|     Laptop|Electronics|50000|     4|       1|     50000|\n",
            "|        2| Smartphone|Electronics|30000|     2|       1|     30000|\n",
            "|        2| Smartphone|Electronics|30000|     6|       2|     60000|\n",
            "|        3|      Table|  Furniture|15000|     3|       3|     45000|\n",
            "|        3|      Table|  Furniture|15000|     8|       1|     15000|\n",
            "|        4|      Chair|  Furniture| 5000|     5|       5|     25000|\n",
            "|        5| Headphones|Electronics| 2000|     7|      10|     20000|\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "\n",
            "Total Sales by Product Category:\n",
            "+-----------+------------------+\n",
            "|   Category|TotalSalesCategory|\n",
            "+-----------+------------------+\n",
            "|Electronics|            260000|\n",
            "|  Furniture|             85000|\n",
            "+-----------+------------------+\n",
            "\n",
            "Sorted Products by Total Sales Value:\n",
            "+-----------+-----------------+\n",
            "|ProductName|TotalSalesProduct|\n",
            "+-----------+-----------------+\n",
            "|     Laptop|           150000|\n",
            "| Smartphone|            90000|\n",
            "|      Table|            60000|\n",
            "|      Chair|            25000|\n",
            "| Headphones|            20000|\n",
            "+-----------+-----------------+\n",
            "\n",
            "Number of Sales for Each Product:\n",
            "+-----------+----------+\n",
            "|ProductName|SalesCount|\n",
            "+-----------+----------+\n",
            "|      Chair|         1|\n",
            "|     Laptop|         2|\n",
            "|      Table|         2|\n",
            "| Smartphone|         2|\n",
            "| Headphones|         1|\n",
            "+-----------+----------+\n",
            "\n",
            "Products with Total Sales Value Greater Than ₹50,000:\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "|ProductID|ProductName|   Category|Price|SaleID|Quantity|TotalSales|\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "|        1|     Laptop|Electronics|50000|     1|       2|    100000|\n",
            "|        2| Smartphone|Electronics|30000|     6|       2|     60000|\n",
            "+---------+-----------+-----------+-----+------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "  .appName(\"RDD Transformation Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#Get the SparkContext from the SparkSession\n",
        "sc = spark.sparkContext\n",
        "print(\"Spark Session Created\")\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "\n",
        "# Print the original RDD\n",
        "print(\"Original RDD:\", rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M57lndsN8AP",
        "outputId": "b8d02562-cc9c-4f34-8c03-22ad303a8d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Created\n",
            "Original RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = rdd.map(lambda x: x* 2)\n",
        "\n",
        "# Print the transformed RDD\n",
        "\n",
        "print(\"RDD after map transformation (x2):\", rdd2.collect())\n",
        "\n",
        "\n",
        "rdd3 = rdd2.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Print the filtered RDD\n",
        "\n",
        "print(\"RDD after filter transformation (even numbers):\", rdd3.collect())\n",
        "\n",
        "\n",
        "\n",
        "sentences = [\"Hello world\", \"PySpark is great\" \"RDD transformations\"]\n",
        "\n",
        "rdd4 = sc.parallelize (sentences)\n",
        "\n",
        "words_rdd = rdd4.flatMap(lambda sentence: sentence.split(\" \"))\n",
        "\n",
        "#Print the flatMapped RDD\n",
        "\n",
        "print(\"RDD after flatMap transformation (split into words):\", words_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xg8N_LWOSjn",
        "outputId": "af0433ab-5e29-48f8-807d-d39079928df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD after map transformation (x2): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
            "RDD after filter transformation (even numbers): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
            "RDD after flatMap transformation (split into words): ['Hello', 'world', 'PySpark', 'is', 'greatRDD', 'transformations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = rdd3.collect()\n",
        "\n",
        "print(results)\n",
        "\n",
        "\n",
        "count = rdd3.count()\n",
        "\n",
        "print(f\"Number of elements: {count}\")\n",
        "\n",
        "\n",
        "total_sum =rdd.reduce(lambda x, y: x + y)\n",
        "print(\"Total sum: (total_sum)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjCcJqUeSkVe",
        "outputId": "e573bab3-4ae3-4622-8519-e598dfbee680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
            "Number of elements: 10\n",
            "Total sum: (total_sum)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating a DataFrame from a dictionary\n",
        "data = {'Name': ['John', 'Emma', 'Alex'],\n",
        "        'Age': [28, 32, 25],\n",
        "        'City': ['New York', 'London', 'Paris']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Creating a DataFrame from a list of lists\n",
        "data = [['John', 28, 'New York'],\n",
        "        ['Emma', 32, 'London'],\n",
        "        ['Alex', 25, 'Paris']]\n",
        "df = pd.DataFrame(data, columns=['Name', 'Age', 'City'])"
      ],
      "metadata": {
        "id": "TpLVehiEyx-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Get basic information about the DataFrame\n",
        "print(df.info())\n",
        "\n",
        "# Get statistical summary of numerical columns\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jFaFNrAy0Yj",
        "outputId": "46639526-6ecc-49e9-8c0c-c64c0e37287b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Name  Age      City\n",
            "0  John   28  New York\n",
            "1  Emma   32    London\n",
            "2  Alex   25     Paris\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3 entries, 0 to 2\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Name    3 non-null      object\n",
            " 1   Age     3 non-null      int64 \n",
            " 2   City    3 non-null      object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 200.0+ bytes\n",
            "None\n",
            "             Age\n",
            "count   3.000000\n",
            "mean   28.333333\n",
            "std     3.511885\n",
            "min    25.000000\n",
            "25%    26.500000\n",
            "50%    28.000000\n",
            "75%    30.000000\n",
            "max    32.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a single column\n",
        "ages = df['Age']\n",
        "# display the column\n",
        "print(ages)\n",
        "\n",
        "# Select multiple columns\n",
        "subset = df[['Name', 'City']]\n",
        "print(subset)\n",
        "# Select rows based on index\n",
        "first_two = df.loc[0:1]\n",
        "print(first_two)"
      ],
      "metadata": {
        "id": "EPvhLLqjy4jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a329118-a67b-41ba-e809-2556bd7d22eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    28\n",
            "1    32\n",
            "2    25\n",
            "Name: Age, dtype: int64\n",
            "   Name      City\n",
            "0  John  New York\n",
            "1  Emma    London\n",
            "2  Alex     Paris\n",
            "   Name  Age      City\n",
            "0  John   28  New York\n",
            "1  Emma   32    London\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns\n",
        "df = df.rename(columns={'Name': 'Full Name', 'City': 'Location'})\n",
        "print(df)"
      ],
      "metadata": {
        "id": "g5yYoq_Sy82B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows based on a condition\n",
        "young_people = df[df['Age'] < 30]\n",
        "print(young_people)\n",
        "# Multiple conditions\n",
        "young_new_yorkers = df[(df['Age'] < 30) & (df['City'] == 'New York')]\n",
        "print(young_new_yorkers)"
      ],
      "metadata": {
        "id": "MKBLbXgAzCBu",
        "outputId": "88568b65-b95a-4ff5-a193-8649c858ac85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Name  Age      City\n",
            "0  John   28  New York\n",
            "2  Alex   25     Paris\n",
            "   Name  Age      City\n",
            "0  John   28  New York\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sep 04"
      ],
      "metadata": {
        "id": "A5zFLHgktaPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Employee Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample employee data\n",
        "data = [\n",
        "    (1, 'Arjun', 'IT', 75000),\n",
        "    (2, 'Vijay', 'Finance', 85000),\n",
        "    (3, 'Shalini', 'IT', 90000),\n",
        "    (4, 'Sneha', 'HR', 50000),\n",
        "    (5, 'Rahul', 'Finance', 60000),\n",
        "    (6, 'Amit', 'IT', 55000)\n",
        "]\n",
        "\n",
        "# Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "# Create DataFrame\n",
        "employee_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "employee_df.show()\n",
        "\n",
        "# Filter employees who have a salary greater than 60,000\n",
        "high_salary_df = employee_df.filter(col('Salary') > 60000)\n",
        "\n",
        "# Calculate the average salary by department\n",
        "avg_salary_df = employee_df.groupBy(\"Department\").avg(\"Salary\")\n",
        "avg_salary_df.show()\n",
        "\n",
        "# Sort employees in descending order of salary\n",
        "sorted_df = employee_df.orderBy(col(\"Salary\").desc())\n",
        "sorted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRG_gq_-sPro",
        "outputId": "12ffa2a8-9f8c-4e5a-94f6-08a0074abde8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "|         5|       Rahul|   Finance| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+-----------------+\n",
            "|Department|      avg(Salary)|\n",
            "+----------+-----------------+\n",
            "|   Finance|          72500.0|\n",
            "|        IT|73333.33333333333|\n",
            "|        HR|          50000.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         5|       Rahul|   Finance| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "+----------+------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Employee Data Handling\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample employee data with null values\n",
        "data = [\n",
        "    (1, 'Arjun', 'IT', 75000),\n",
        "    (2, 'Vijay', 'Finance', 85000),\n",
        "    (3, None, 'IT', 90000),\n",
        "    (4, 'Sneha', 'HR', None),\n",
        "    (5, 'Rahul', None, 60000),\n",
        "    (6, 'Amit', 'IT', 55000)\n",
        "]\n",
        "\n",
        "# Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "# Create DataFrame\n",
        "employee_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "employee_df.show()\n",
        "\n",
        "# Fill null values in 'EmployeeName' and 'Department' with 'Unknown'\n",
        "filled_df = employee_df.fillna({'EmployeeName': 'Unknown', 'Department': 'Unknown'})\n",
        "filled_df.show()\n",
        "\n",
        "# Drop rows where 'Salary' is null\n",
        "dropped_null_salary_df = employee_df.dropna(subset=['Salary'])\n",
        "dropped_null_salary_df.show()\n",
        "\n",
        "# Fill null values in 'Salary' with 50000\n",
        "salary_filled_df = employee_df.fillna({'Salary': 50000})\n",
        "salary_filled_df.show()\n",
        "\n",
        "# Check for null values in the entire DataFrame\n",
        "null_counts = employee_df.select([col(c).isNull().alias(c) for c in employee_df.columns]).show()\n",
        "\n",
        "# Replace all null values in the DataFrame with 'N/A'\n",
        "na_filled_df = employee_df.na.fill('N/A')\n",
        "na_filled_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C1cCk9j3Vlp",
        "outputId": "5a748a15-8be3-4bbd-b6a3-280a68df69a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Unknown|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|   Unknown| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|     false|       false|     false| false|\n",
            "|     false|       false|     false| false|\n",
            "|     false|        true|     false| false|\n",
            "|     false|       false|     false|  true|\n",
            "|     false|       false|      true| false|\n",
            "|     false|       false|     false| false|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|         N/A|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|       N/A| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, col, sum\n",
        "\n",
        "# Initialize a Spark Session\n",
        "spark = SparkSession.builder\\\n",
        "    .appName(\"Advanced Dataframe Operations\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data1 = [\n",
        "    (1, 'Arjun', 'IT', 75000, '2022-01-15'),\n",
        "    (2, 'Vijay', 'Finance', 85000, '2022-03-12'),\n",
        "    (3, 'Shalini', 'IT', 90000, '2021-06-30')\n",
        "]\n",
        "\n",
        "data2 = [\n",
        "    (4, 'Sneha', 'HR', 50000, '2022-05-01'),\n",
        "    (5, 'Rahul', 'Finance', 60000, '2022-08-20'),\n",
        "    (6, 'Amit', 'IT', 55000, '2021-12-15')\n",
        "]\n",
        "\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary', 'JoiningDate']\n",
        "\n",
        "# Create DataFrames\n",
        "employee_df1 = spark.createDataFrame(data=data1, schema=columns)\n",
        "employee_df2 = spark.createDataFrame(data=data2, schema=columns)\n",
        "\n",
        "employee_df1.show()\n",
        "employee_df2.show()\n",
        "\n",
        "# Union two DataFrames (removes duplicates)\n",
        "union_df = employee_df1.union(employee_df2).dropDuplicates()\n",
        "print(\"Union of two dataframes (Remove duplicates): \")\n",
        "union_df.show()\n",
        "\n",
        "# Union of two DataFrames (includes everything)\n",
        "union_all_df = employee_df1.union(employee_df2)\n",
        "print(\"Union of two dataframes: \")\n",
        "union_all_df.show()\n",
        "\n",
        "# Rank employees by salary within each department\n",
        "window_spec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
        "ranked_df = union_all_df.withColumn(\"Rank\", rank().over(window_spec))\n",
        "ranked_df.show()\n",
        "\n",
        "# Convert 'JoiningDate' from string to date type\n",
        "date_converted_df = union_all_df.withColumn(\"JoiningDate\", F.to_date(col(\"JoiningDate\"), \"yyyy-MM-dd\"))\n",
        "date_converted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2CAOF0m6wiI",
        "outputId": "6772706e-a798-4ed4-cf76-1ec0426904f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "Union of two dataframes (Remove duplicates): \n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "Union of two dataframes: \n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|Rank|\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|   1|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|   2|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|   1|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|   1|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|   2|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|   3|\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "# Define a window specification for cumulative sum of salaries within each department\n",
        "window_spec_sum = Window.partitionBy(\"Department\").orderBy(\"JoiningDate\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Calculate the running total of salaries\n",
        "running_total_df = date_converted_df.withColumn(\"RunningTotal\", sum(col(\"Salary\")).over(window_spec_sum))\n",
        "running_total_df.show()\n",
        "\n",
        "# Calculate the number of years since joining\n",
        "experience_df = running_total_df.withColumn(\"YearsOfExperience\", F.round(F.datediff(F.current_date(), col(\"JoiningDate\")) / 365, 2))\n",
        "experience_df.show()\n",
        "\n",
        "# Add a new column for next evaluation date (one year after joining)\n",
        "eval_date_df = experience_df.withColumn(\"NextEvaluationDate\", F.date_add(col(\"JoiningDate\"), 365))\n",
        "eval_date_df.show()\n",
        "\n",
        "# Calculate average salary per department\n",
        "avg_salary_df = union_all_df.groupBy(\"Department\").agg(F.avg(\"Salary\").alias(\"AverageSalary\"))\n",
        "avg_salary_df.show()\n",
        "\n",
        "# Calculate the total number of employees\n",
        "total_employees_df = union_all_df.agg(F.count(\"EmployeeID\").alias(\"TotalEmployees\"))\n",
        "total_employees_df.show()\n",
        "\n",
        "# Convert employee names to uppercase\n",
        "uppercase_name_df = union_all_df.withColumn(\"EmployeeNameUpper\", F.upper(col(\"EmployeeName\")))\n",
        "uppercase_name_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcmy0OSFS3ev",
        "outputId": "5b84ca33-37ec-4341-a7b0-dbf0f0cb0843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+-----------+------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|RunningTotal|\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|       85000|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|      145000|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|       50000|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|       90000|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|      145000|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|      220000|\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|RunningTotal|YearsOfExperience|\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|       85000|             2.48|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|      145000|             2.04|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|       50000|             2.35|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|       90000|             3.18|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|      145000|             2.72|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|      220000|             2.64|\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+------------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|RunningTotal|YearsOfExperience|NextEvaluationDate|\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+------------------+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|       85000|             2.48|        2023-03-12|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|      145000|             2.04|        2023-08-20|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|       50000|             2.35|        2023-05-01|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|       90000|             3.18|        2022-06-30|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|      145000|             2.72|        2022-12-15|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|      220000|             2.64|        2023-01-15|\n",
            "+----------+------------+----------+------+-----------+------------+-----------------+------------------+\n",
            "\n",
            "+----------+-----------------+\n",
            "|Department|    AverageSalary|\n",
            "+----------+-----------------+\n",
            "|        IT|73333.33333333333|\n",
            "|   Finance|          72500.0|\n",
            "|        HR|          50000.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "+--------------+\n",
            "|TotalEmployees|\n",
            "+--------------+\n",
            "|             6|\n",
            "+--------------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|EmployeeNameUpper|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|            ARJUN|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|            VIJAY|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|          SHALINI|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|            SNEHA|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|            RAHUL|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|             AMIT|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wP3E8WLVTYO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}