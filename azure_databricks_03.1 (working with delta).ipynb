{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c121a7b-0e25-4dec-9abf-a38c4cbbd9a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee_data.csv\", \"dbfs:/FileStore/employee_data.csv\")\n",
    "\n",
    "# Load CSV data into a DataFrame\n",
    "df_employee = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/employee_data.csv\")\n",
    "\n",
    "# Write DataFrame to Delta format with overwrite option\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0fa5a08-5f21-45f9-a414-b59a680df789",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+------+-----+\n|ProductID|ProductName|   Category| Price|Stock|\n+---------+-----------+-----------+------+-----+\n|      101|     Laptop|Electronics|1200.0|   35|\n|      102| Smartphone|Electronics| 800.0|   80|\n|      103| Desk Chair|  Furniture| 150.0|   60|\n|      104|    Monitor|Electronics| 300.0|   45|\n|      105|       Desk|  Furniture| 350.0|   25|\n+---------+-----------+-----------+------+-----+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Define schema for JSON file\n",
    "schema = StructType([\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"Stock\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Move the file from Workspace to DBFS \n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/product_data.json\", \"dbfs:/FileStore/product_data.json\")\n",
    "\n",
    "# Load JSON data with schema\n",
    "df_product = spark.read.format(\"json\").schema(schema).load(\"dbfs:/FileStore/product_data.json\")\n",
    "df_product.show()\n",
    "\n",
    "# Create a temp view for SQL operations\n",
    "df_product.createOrReplaceTempView(\"product_view\")\n",
    "\n",
    "# Create a Delta table from the view if it does not already exist\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS delta_product_table_\n",
    "    USING DELTA\n",
    "    AS SELECT * FROM product_view\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d7e65b-5cdd-401e-be8e-30771ddd93d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[ProductID: int, ProductName: string, Category: string, Price: double, Stock: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM delta_product_table_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44044842-2a63-48be-9216-e116b9b2a130",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "employee_updates.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57699c57-32f3-4d10-acce-3823e5ee1dc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee_updates.csv\", \"dbfs:/FileStore/employee_updates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4039b6-0a81-46ad-92b6-e1d8ba9a8481",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert employee CSV data to Delta format\n",
    "df_employee = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/employee_updates.csv\")\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238574bd-308a-4a51-a728-10dc3ba8ad9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert employee updates CSV data to Delta format\n",
    "df_employee_updates = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/employee_updates.csv\")\n",
    "df_employee_updates.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff048a5-cd92-4521-acdf-831adf775d25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Delta tables\n",
    "df_employee = spark.read.format(\"delta\").load(\"/delta/employee_data\")\n",
    "df_employee_updates = spark.read.format(\"delta\").load(\"/delta/employee_updates\")\n",
    "\n",
    "# Create temporary views for SQL operations\n",
    "df_employee.createOrReplaceTempView(\"delta_employee\")\n",
    "df_employee_updates.createOrReplaceTempView (\"employee_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2f7154-cefb-4cb0-b549-f2003423311a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        MERGE INTO delta_employee AS target\n",
    "        USING employee_updates AS source\n",
    "        ON target.EmployeeID = source.EmployeeID\n",
    "        WHEN MATCHED THEN UPDATE SET target.Salary = source.Salary, target.Department = source.Department\n",
    "        WHEN NOT MATCHED THEN INSERT (EmployeeID, Name, Department, JoiningDate, Salary)\n",
    "        VALUES (source.EmployeeID, source.Name, source.Department, source.JoiningDate, source.Salary)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7933c7-7e8a-4c1e-aabd-7051b0753f01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+------+\n|EmployeeID|       Name|Department|JoiningDate|Salary|\n+----------+-----------+----------+-----------+------+\n|      1001|   John Doe|        HR| 2021-01-15| 58000|\n|      1009|Sarah Adams| Marketing| 2021-09-01| 60000|\n|      1010|Robert King|        IT| 2022-01-10| 62000|\n+----------+-----------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM delta_employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf0a146-0c75-4653-ac57-6e58038313dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the employee DataFrame to a Delta table\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"delta/employee_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c93210f7-57bb-4c77-bbeb-fb7f95d43035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the Delta Table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_employee_table USING DELTA LOCATION '/delta/employee_data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fed898c-a937-4a18-86c7-a22da55563e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize the Delta Table\n",
    "spark.sql(\"OPTIMIZE delta_employee_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775caad6-f832-4ac3-b295-e330202ab97c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1002|   Jane Smith|        IT| 2020-03-10| 65100|\n|      1005| David Wilson|        IT| 2021-06-25| 60900|\n|      1007| James Miller|        IT| 2019-08-14| 68250|\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM delta_employee_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb42fe4-3613-494e-9dae-b0d5987c8f12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation             |operationParameters                                                                                                                    |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                            |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|2      |2024-09-12 07:15:22|6016586593975937|azuser2134_mml.local@techademy.com|OPTIMIZE              |{predicate -> [], zOrderBy -> [], batchId -> 0, auto -> true}                                                                          |NULL|{2456164254236261}|0912-035714-9put9a6m|1          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 2788, p25FileSize -> 1483, numDeletionVectorsRemoved -> 1, minFileSize -> 1483, numAddedFiles -> 1, maxFileSize -> 1483, p75FileSize -> 1483, p50FileSize -> 1483, numAddedBytes -> 1483}                                                                                         |NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n|1      |2024-09-12 07:15:20|6016586593975937|azuser2134_mml.local@techademy.com|UPDATE                |{predicate -> [\"(Department#7215 = IT)\"]}                                                                                              |NULL|{2456164254236261}|0912-035714-9put9a6m|0          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 2929, numDeletionVectorsUpdated -> 0, scanTimeMs -> 1374, numAddedFiles -> 1, numUpdatedRows -> 3, numAddedBytes -> 1305, rewriteTimeMs -> 1496}|NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n|0      |2024-09-12 07:00:51|6016586593975937|azuser2134_mml.local@techademy.com|CREATE TABLE AS SELECT|{partitionBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{2456164254236261}|0912-035714-9put9a6m|NULL       |WriteSerializable|true         |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1483}                                                                                                                                                                                                                                                                 |NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Describe the history of the Delta Table\n",
    "spark.sql(\"DESCRIBE HISTORY delta_employee_table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b709f6-7320-4183-91c5-3e50f07c7b88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"OPTIMIZE delta_employee_table ZORDER BY Department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81150af2-8f7e-4b1e-bbd3-132a3b45044c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"VACUUM delta_employee_table RETAIN 168 HOURS\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "azure_databricks_03.1 (working with delta)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
