{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "822a8f4d-7f79-4813-83e8-6479d4dfdd22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Assignment 1: Working with CSV Data (employee_data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69651cc1-6603-41b0-931a-e3f3fbdadc6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of employee data:\n+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\nEmployee data schema:\nroot\n |-- EmployeeID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- JoiningDate: date (nullable = true)\n |-- Salary: integer (nullable = true)\n\nAverage salary by department:\n+----------+---------+\n|Department|AvgSalary|\n+----------+---------+\n|        HR|  55000.0|\n|        IT|  58000.0|\n+----------+---------+\n\nEmployee count by department:\n+----------+-------------+\n|Department|EmployeeCount|\n+----------+-------------+\n|        HR|            1|\n|        IT|            1|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, avg, sum, count\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()\n",
    "\n",
    "\n",
    "# 1. Load the CSV data\n",
    "df_employee = spark.read.csv(\"file:/Workspace/Shared/employee_data.csv\", header=True, inferSchema=True)\n",
    "print(\"First 10 rows of employee data:\")\n",
    "df_employee.show(10)\n",
    "print(\"Employee data schema:\")\n",
    "df_employee.printSchema()\n",
    "\n",
    "# 2. Data Cleaning\n",
    "# Remove rows where the Salary is less than 55,000.\n",
    "# Filter the employees who joined after the year 2020.\n",
    "\n",
    "df_employee_cleaned = df_employee.filter(col(\"Salary\") >= 55000) \\\n",
    "  .filter(year(col(\"JoiningDate\")) > 2020)\n",
    "\n",
    "# 3. Data Aggregation\n",
    "# Find the average salary by Department.\n",
    "\n",
    "avg_salary_by_dept = df_employee_cleaned.groupBy(\"Department\") \\\n",
    "  .agg(avg(\"Salary\").alias(\"AvgSalary\"))\n",
    "print(\"Average salary by department:\")\n",
    "avg_salary_by_dept.show()\n",
    "\n",
    "# Count the number of employees in each Department.\n",
    "\n",
    "employee_count_by_dept = df_employee_cleaned.groupBy(\"Department\") \\\n",
    "  .agg(count(\"EmployeeID\").alias(\"EmployeeCount\"))\n",
    "print(\"Employee count by department:\")\n",
    "employee_count_by_dept.show()\n",
    "\n",
    "# 4. Write the Data to CSV\n",
    "# Save the cleaned data (from the previous steps) to a new CSV file.\n",
    "\n",
    "df_employee_cleaned.write.csv(\"/Workspace/Shared/cleaned_employee_data.csv\", header=True, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4533cc21-9e61-4cf1-9d39-ca880557962b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assignment 2: Working with JSON Data (product_data.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "797604c1-7506-43f2-a7ea-6bba2f256ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|  Furniture|  150|      103| Desk Chair|   60|\n|Electronics|  300|      104|    Monitor|   45|\n|  Furniture|  350|      105|       Desk|   25|\n+-----------+-----+---------+-----------+-----+\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|Electronics|  300|      104|    Monitor|   45|\n+-----------+-----+---------+-----------+-----+\n\n+-------------------+\n|TotalFurnitureStock|\n+-------------------+\n|                 85|\n+-------------------+\n\n+------------+\n|AveragePrice|\n+------------+\n|       560.0|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/product_data.json\", \"dbfs:/FileStore/product_data.json\")\n",
    "\n",
    "product_df = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/product_data.json\")\n",
    "\n",
    "# First 10 rows\n",
    "product_df.show(10)\n",
    "\n",
    "# Data Cleaning\n",
    "# Remove rows where stock is less than 30\n",
    "product_df_cleaned = product_df.filter(col(\"Stock\") >= 30)\n",
    "\n",
    "# Filter products in Electronics category\n",
    "df_electronics = product_df_cleaned.filter(col(\"Category\") == \"Electronics\")\n",
    "df_electronics.show()\n",
    "\n",
    "# Data Aggregation\n",
    "#  Calculate the total stock for products in the \"Furniture\" category.\n",
    "df_furniture = product_df.filter(col(\"Category\") == \"Furniture\").agg(F.sum(\"Stock\").alias(\"TotalFurnitureStock\"))\n",
    "df_furniture.show()\n",
    "\n",
    "#  Find the average price of all products in the dataset.\n",
    "df_avg = product_df.agg(F.avg(\"Price\").alias(\"AveragePrice\"))\n",
    "df_avg.show()\n",
    "\n",
    "# Write to a Json file\n",
    "df_electronics.write.format(\"json\").mode(\"overwrite\").save(\"file:/Workspace/Shared/product_data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28cfec9c-419f-430d-a761-142abf8df672",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assignment 3: Working with Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b68b554-2e72-4e22-a55e-2df0d242fcc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products before delete operation\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|  Furniture|  150|      103| Desk Chair|   60|\n|Electronics|  300|      104|    Monitor|   45|\n|  Furniture|  350|      105|       Desk|   25|\n+-----------+-----+---------+-----------+-----+\n\nEmployees before update operation\n+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\n+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n+----------+-------------+----------+-----------+------+\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics|  800|      102| Smartphone|   80|\n+-----------+-----+---------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee_data.csv\", \"dbfs:/FileStore/employee_data.csv\")\n",
    "\n",
    "product_df = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/product_data.json\") \n",
    "\n",
    "employee_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/employee_data.csv\")\n",
    "\n",
    "# Converting CSV and JSON to delta format\n",
    "\n",
    "employee_df.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Shared/employee_table\")\n",
    "\n",
    "product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Shared/product_table\")\n",
    "\n",
    "# Register Delta Tables\n",
    "\n",
    "# employee_df.write.saveAsTable(\"employee_table\")\n",
    "# product_df.write.saveAsTable(\"products_table\")\n",
    "\n",
    "employee_delta_path = \"/Workspace/Shared/employee_table\"\n",
    "product_delta_path = \"/Workspace/Shared/product_table\"\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS employee_delta USING DELTA LOCATION '{employee_delta_path}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS product_delta USING DELTA LOCATION '{product_delta_path}'\")\n",
    "\n",
    "# Data modification\n",
    "\n",
    "# Increase the salary by 5% for all employees in the IT department.\n",
    "spark.sql(\"update employee_delta set Salary = Salary * 1.05 where Department = 'IT'\")\n",
    "\n",
    "#  Delete products where the stock is less than 40.\n",
    "spark.sql(\"delete from product_delta where Stock < 40\")\n",
    "\n",
    "# Time Travel \n",
    "\n",
    "print(\"Products before delete operation\")\n",
    "spark.sql(\"SELECT * FROM product_delta VERSION AS OF 0\").show()\n",
    "\n",
    "print(\"Employees before update operation\")\n",
    "spark.sql(\"SELECT * FROM employee_delta VERSION AS OF 0\").show()\n",
    "\n",
    "# Query Delta Tables\n",
    "\n",
    "# Query employees in finance department\n",
    "spark.sql(\"SELECT * FROM employee_delta WHERE Department = 'Finance'\").show()\n",
    "\n",
    "# Query Electronics products with price > 500\n",
    "spark.sql(\"SELECT * FROM product_delta WHERE Category = 'Electronics' AND Price > 500\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Exercises Sep 12",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
